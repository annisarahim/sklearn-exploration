{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Made by:\n",
    "- Annisa Rahim / 13518089\n",
    "- Stefanus Gusega Gunawan / 13518149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a dict to record the evaluation of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "  \n",
    "# initialise data of lists. \n",
    "data = {'algo':['DecisionTreeClassifier', 'Id3Estimator', 'KMeans','Logreg','Logreg_StdScaler', 'Logreg_MinMaxScaler','NN',\n",
    "               'NN_StdScaler','NN_MinMaxScaler','SVM'], \n",
    "        'acc_train':[],\n",
    "        'acc_valid':[],\n",
    "        'f1_train':[],\n",
    "        'f1_valid':[]\n",
    "       } \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast-Cancer Dataset\n",
    "\n",
    "Load the datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>14.690</td>\n",
       "      <td>13.98</td>\n",
       "      <td>98.22</td>\n",
       "      <td>656.1</td>\n",
       "      <td>0.10310</td>\n",
       "      <td>0.18360</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.07406</td>\n",
       "      <td>...</td>\n",
       "      <td>16.460</td>\n",
       "      <td>18.34</td>\n",
       "      <td>114.10</td>\n",
       "      <td>809.2</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.36350</td>\n",
       "      <td>0.32190</td>\n",
       "      <td>0.11080</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.09208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>13.170</td>\n",
       "      <td>18.66</td>\n",
       "      <td>85.98</td>\n",
       "      <td>534.6</td>\n",
       "      <td>0.11580</td>\n",
       "      <td>0.12310</td>\n",
       "      <td>0.12260</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.06777</td>\n",
       "      <td>...</td>\n",
       "      <td>15.670</td>\n",
       "      <td>27.95</td>\n",
       "      <td>102.80</td>\n",
       "      <td>759.4</td>\n",
       "      <td>0.17860</td>\n",
       "      <td>0.41660</td>\n",
       "      <td>0.50060</td>\n",
       "      <td>0.20880</td>\n",
       "      <td>0.3900</td>\n",
       "      <td>0.11790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>12.950</td>\n",
       "      <td>16.02</td>\n",
       "      <td>83.14</td>\n",
       "      <td>513.7</td>\n",
       "      <td>0.10050</td>\n",
       "      <td>0.07943</td>\n",
       "      <td>0.06155</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.06470</td>\n",
       "      <td>...</td>\n",
       "      <td>13.740</td>\n",
       "      <td>19.93</td>\n",
       "      <td>88.81</td>\n",
       "      <td>585.4</td>\n",
       "      <td>0.14830</td>\n",
       "      <td>0.20680</td>\n",
       "      <td>0.22410</td>\n",
       "      <td>0.10560</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.09584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>18.310</td>\n",
       "      <td>18.58</td>\n",
       "      <td>118.60</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>0.08588</td>\n",
       "      <td>0.08468</td>\n",
       "      <td>0.08169</td>\n",
       "      <td>0.058140</td>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.05425</td>\n",
       "      <td>...</td>\n",
       "      <td>21.310</td>\n",
       "      <td>26.36</td>\n",
       "      <td>139.20</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>0.12340</td>\n",
       "      <td>0.24450</td>\n",
       "      <td>0.35380</td>\n",
       "      <td>0.15710</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>0.06938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>15.130</td>\n",
       "      <td>29.81</td>\n",
       "      <td>96.71</td>\n",
       "      <td>719.5</td>\n",
       "      <td>0.08320</td>\n",
       "      <td>0.04605</td>\n",
       "      <td>0.04686</td>\n",
       "      <td>0.027390</td>\n",
       "      <td>0.1852</td>\n",
       "      <td>0.05294</td>\n",
       "      <td>...</td>\n",
       "      <td>17.260</td>\n",
       "      <td>36.91</td>\n",
       "      <td>110.10</td>\n",
       "      <td>931.4</td>\n",
       "      <td>0.11480</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.15470</td>\n",
       "      <td>0.06575</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.06165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>16.160</td>\n",
       "      <td>21.54</td>\n",
       "      <td>106.20</td>\n",
       "      <td>809.8</td>\n",
       "      <td>0.10080</td>\n",
       "      <td>0.12840</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.056130</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.05891</td>\n",
       "      <td>...</td>\n",
       "      <td>19.470</td>\n",
       "      <td>31.68</td>\n",
       "      <td>129.70</td>\n",
       "      <td>1175.0</td>\n",
       "      <td>0.13950</td>\n",
       "      <td>0.30550</td>\n",
       "      <td>0.29920</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.3480</td>\n",
       "      <td>0.07619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>19.190</td>\n",
       "      <td>15.94</td>\n",
       "      <td>126.30</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>0.08694</td>\n",
       "      <td>0.11850</td>\n",
       "      <td>0.11930</td>\n",
       "      <td>0.096670</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.05176</td>\n",
       "      <td>...</td>\n",
       "      <td>22.030</td>\n",
       "      <td>17.81</td>\n",
       "      <td>146.60</td>\n",
       "      <td>1495.0</td>\n",
       "      <td>0.11240</td>\n",
       "      <td>0.20160</td>\n",
       "      <td>0.22640</td>\n",
       "      <td>0.17770</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>0.06251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>18.080</td>\n",
       "      <td>21.84</td>\n",
       "      <td>117.40</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>0.07371</td>\n",
       "      <td>0.08642</td>\n",
       "      <td>0.11030</td>\n",
       "      <td>0.057780</td>\n",
       "      <td>0.1770</td>\n",
       "      <td>0.05340</td>\n",
       "      <td>...</td>\n",
       "      <td>19.760</td>\n",
       "      <td>24.70</td>\n",
       "      <td>129.10</td>\n",
       "      <td>1228.0</td>\n",
       "      <td>0.08822</td>\n",
       "      <td>0.19630</td>\n",
       "      <td>0.25350</td>\n",
       "      <td>0.09181</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>0.06558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>10.480</td>\n",
       "      <td>19.86</td>\n",
       "      <td>66.72</td>\n",
       "      <td>337.7</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.05971</td>\n",
       "      <td>0.04831</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.06440</td>\n",
       "      <td>...</td>\n",
       "      <td>11.480</td>\n",
       "      <td>29.46</td>\n",
       "      <td>73.68</td>\n",
       "      <td>402.8</td>\n",
       "      <td>0.15150</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.11810</td>\n",
       "      <td>0.06736</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>0.07748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>14.530</td>\n",
       "      <td>13.98</td>\n",
       "      <td>93.86</td>\n",
       "      <td>644.2</td>\n",
       "      <td>0.10990</td>\n",
       "      <td>0.09242</td>\n",
       "      <td>0.06895</td>\n",
       "      <td>0.064950</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.06121</td>\n",
       "      <td>...</td>\n",
       "      <td>15.800</td>\n",
       "      <td>16.93</td>\n",
       "      <td>103.10</td>\n",
       "      <td>749.9</td>\n",
       "      <td>0.13470</td>\n",
       "      <td>0.14780</td>\n",
       "      <td>0.13730</td>\n",
       "      <td>0.10690</td>\n",
       "      <td>0.2606</td>\n",
       "      <td>0.07810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>11.040</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.1714</td>\n",
       "      <td>0.06340</td>\n",
       "      <td>...</td>\n",
       "      <td>12.410</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.13690</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>14.190</td>\n",
       "      <td>23.81</td>\n",
       "      <td>92.87</td>\n",
       "      <td>610.7</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.13060</td>\n",
       "      <td>0.11150</td>\n",
       "      <td>0.064620</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.06433</td>\n",
       "      <td>...</td>\n",
       "      <td>16.860</td>\n",
       "      <td>34.85</td>\n",
       "      <td>115.00</td>\n",
       "      <td>811.3</td>\n",
       "      <td>0.15590</td>\n",
       "      <td>0.40590</td>\n",
       "      <td>0.37440</td>\n",
       "      <td>0.17720</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.10260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>16.240</td>\n",
       "      <td>18.77</td>\n",
       "      <td>108.80</td>\n",
       "      <td>805.1</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.18020</td>\n",
       "      <td>0.19480</td>\n",
       "      <td>0.090520</td>\n",
       "      <td>0.1876</td>\n",
       "      <td>0.06684</td>\n",
       "      <td>...</td>\n",
       "      <td>18.550</td>\n",
       "      <td>25.09</td>\n",
       "      <td>126.90</td>\n",
       "      <td>1031.0</td>\n",
       "      <td>0.13650</td>\n",
       "      <td>0.47060</td>\n",
       "      <td>0.50260</td>\n",
       "      <td>0.17320</td>\n",
       "      <td>0.2770</td>\n",
       "      <td>0.10630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>12.360</td>\n",
       "      <td>18.54</td>\n",
       "      <td>79.01</td>\n",
       "      <td>466.7</td>\n",
       "      <td>0.08477</td>\n",
       "      <td>0.06815</td>\n",
       "      <td>0.02643</td>\n",
       "      <td>0.019210</td>\n",
       "      <td>0.1602</td>\n",
       "      <td>0.06066</td>\n",
       "      <td>...</td>\n",
       "      <td>13.290</td>\n",
       "      <td>27.49</td>\n",
       "      <td>85.56</td>\n",
       "      <td>544.1</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.19630</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.08442</td>\n",
       "      <td>0.2983</td>\n",
       "      <td>0.07185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>14.740</td>\n",
       "      <td>25.42</td>\n",
       "      <td>94.70</td>\n",
       "      <td>668.6</td>\n",
       "      <td>0.08275</td>\n",
       "      <td>0.07214</td>\n",
       "      <td>0.04105</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.05680</td>\n",
       "      <td>...</td>\n",
       "      <td>16.510</td>\n",
       "      <td>32.29</td>\n",
       "      <td>107.40</td>\n",
       "      <td>826.4</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.13760</td>\n",
       "      <td>0.16110</td>\n",
       "      <td>0.10950</td>\n",
       "      <td>0.2722</td>\n",
       "      <td>0.06956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>12.880</td>\n",
       "      <td>18.22</td>\n",
       "      <td>84.45</td>\n",
       "      <td>493.1</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.16610</td>\n",
       "      <td>0.04825</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.07253</td>\n",
       "      <td>...</td>\n",
       "      <td>15.050</td>\n",
       "      <td>24.37</td>\n",
       "      <td>99.31</td>\n",
       "      <td>674.7</td>\n",
       "      <td>0.14560</td>\n",
       "      <td>0.29610</td>\n",
       "      <td>0.12460</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.2582</td>\n",
       "      <td>0.08893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>12.200</td>\n",
       "      <td>15.21</td>\n",
       "      <td>78.01</td>\n",
       "      <td>457.9</td>\n",
       "      <td>0.08673</td>\n",
       "      <td>0.06545</td>\n",
       "      <td>0.01994</td>\n",
       "      <td>0.016920</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>0.06129</td>\n",
       "      <td>...</td>\n",
       "      <td>13.750</td>\n",
       "      <td>21.38</td>\n",
       "      <td>91.11</td>\n",
       "      <td>583.1</td>\n",
       "      <td>0.12560</td>\n",
       "      <td>0.19280</td>\n",
       "      <td>0.11670</td>\n",
       "      <td>0.05556</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>0.07961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>11.680</td>\n",
       "      <td>16.17</td>\n",
       "      <td>75.49</td>\n",
       "      <td>420.5</td>\n",
       "      <td>0.11280</td>\n",
       "      <td>0.09263</td>\n",
       "      <td>0.04279</td>\n",
       "      <td>0.031320</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>0.06401</td>\n",
       "      <td>...</td>\n",
       "      <td>13.320</td>\n",
       "      <td>21.59</td>\n",
       "      <td>86.57</td>\n",
       "      <td>549.8</td>\n",
       "      <td>0.15260</td>\n",
       "      <td>0.14770</td>\n",
       "      <td>0.14900</td>\n",
       "      <td>0.09815</td>\n",
       "      <td>0.2804</td>\n",
       "      <td>0.08024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>10.570</td>\n",
       "      <td>20.22</td>\n",
       "      <td>70.15</td>\n",
       "      <td>338.3</td>\n",
       "      <td>0.09073</td>\n",
       "      <td>0.16600</td>\n",
       "      <td>0.22800</td>\n",
       "      <td>0.059410</td>\n",
       "      <td>0.2188</td>\n",
       "      <td>0.08450</td>\n",
       "      <td>...</td>\n",
       "      <td>10.850</td>\n",
       "      <td>22.82</td>\n",
       "      <td>76.51</td>\n",
       "      <td>351.9</td>\n",
       "      <td>0.11430</td>\n",
       "      <td>0.36190</td>\n",
       "      <td>0.60300</td>\n",
       "      <td>0.14650</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.12000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>20.480</td>\n",
       "      <td>21.46</td>\n",
       "      <td>132.50</td>\n",
       "      <td>1306.0</td>\n",
       "      <td>0.08355</td>\n",
       "      <td>0.08348</td>\n",
       "      <td>0.09042</td>\n",
       "      <td>0.060220</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.05177</td>\n",
       "      <td>...</td>\n",
       "      <td>24.220</td>\n",
       "      <td>26.17</td>\n",
       "      <td>161.70</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>0.12280</td>\n",
       "      <td>0.23110</td>\n",
       "      <td>0.31580</td>\n",
       "      <td>0.14450</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.07127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>12.760</td>\n",
       "      <td>18.84</td>\n",
       "      <td>81.87</td>\n",
       "      <td>496.6</td>\n",
       "      <td>0.09676</td>\n",
       "      <td>0.07952</td>\n",
       "      <td>0.02688</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.1759</td>\n",
       "      <td>0.06183</td>\n",
       "      <td>...</td>\n",
       "      <td>13.750</td>\n",
       "      <td>25.99</td>\n",
       "      <td>87.82</td>\n",
       "      <td>579.7</td>\n",
       "      <td>0.12980</td>\n",
       "      <td>0.18390</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>0.08312</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.07238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>11.570</td>\n",
       "      <td>19.04</td>\n",
       "      <td>74.20</td>\n",
       "      <td>409.7</td>\n",
       "      <td>0.08546</td>\n",
       "      <td>0.07722</td>\n",
       "      <td>0.05485</td>\n",
       "      <td>0.014280</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.06267</td>\n",
       "      <td>...</td>\n",
       "      <td>13.070</td>\n",
       "      <td>26.98</td>\n",
       "      <td>86.43</td>\n",
       "      <td>520.5</td>\n",
       "      <td>0.12490</td>\n",
       "      <td>0.19370</td>\n",
       "      <td>0.25600</td>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.3035</td>\n",
       "      <td>0.08284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>19.590</td>\n",
       "      <td>25.00</td>\n",
       "      <td>127.70</td>\n",
       "      <td>1191.0</td>\n",
       "      <td>0.10320</td>\n",
       "      <td>0.09871</td>\n",
       "      <td>0.16550</td>\n",
       "      <td>0.090630</td>\n",
       "      <td>0.1663</td>\n",
       "      <td>0.05391</td>\n",
       "      <td>...</td>\n",
       "      <td>21.440</td>\n",
       "      <td>30.96</td>\n",
       "      <td>139.80</td>\n",
       "      <td>1421.0</td>\n",
       "      <td>0.15280</td>\n",
       "      <td>0.18450</td>\n",
       "      <td>0.39770</td>\n",
       "      <td>0.14660</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.06091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>12.750</td>\n",
       "      <td>16.70</td>\n",
       "      <td>82.51</td>\n",
       "      <td>493.8</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.11170</td>\n",
       "      <td>0.03880</td>\n",
       "      <td>0.029950</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.06623</td>\n",
       "      <td>...</td>\n",
       "      <td>14.450</td>\n",
       "      <td>21.74</td>\n",
       "      <td>93.63</td>\n",
       "      <td>624.1</td>\n",
       "      <td>0.14750</td>\n",
       "      <td>0.19790</td>\n",
       "      <td>0.14230</td>\n",
       "      <td>0.08045</td>\n",
       "      <td>0.3071</td>\n",
       "      <td>0.08557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>14.780</td>\n",
       "      <td>23.94</td>\n",
       "      <td>97.40</td>\n",
       "      <td>668.3</td>\n",
       "      <td>0.11720</td>\n",
       "      <td>0.14790</td>\n",
       "      <td>0.12670</td>\n",
       "      <td>0.090290</td>\n",
       "      <td>0.1953</td>\n",
       "      <td>0.06654</td>\n",
       "      <td>...</td>\n",
       "      <td>17.310</td>\n",
       "      <td>33.39</td>\n",
       "      <td>114.60</td>\n",
       "      <td>925.1</td>\n",
       "      <td>0.16480</td>\n",
       "      <td>0.34160</td>\n",
       "      <td>0.30240</td>\n",
       "      <td>0.16140</td>\n",
       "      <td>0.3321</td>\n",
       "      <td>0.08911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>11.300</td>\n",
       "      <td>18.19</td>\n",
       "      <td>73.93</td>\n",
       "      <td>389.4</td>\n",
       "      <td>0.09592</td>\n",
       "      <td>0.13250</td>\n",
       "      <td>0.15480</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.2054</td>\n",
       "      <td>0.07669</td>\n",
       "      <td>...</td>\n",
       "      <td>12.580</td>\n",
       "      <td>27.96</td>\n",
       "      <td>87.16</td>\n",
       "      <td>472.9</td>\n",
       "      <td>0.13470</td>\n",
       "      <td>0.48480</td>\n",
       "      <td>0.74360</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.3308</td>\n",
       "      <td>0.12970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>14.590</td>\n",
       "      <td>22.68</td>\n",
       "      <td>96.39</td>\n",
       "      <td>657.1</td>\n",
       "      <td>0.08473</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.10290</td>\n",
       "      <td>0.037360</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>0.06147</td>\n",
       "      <td>...</td>\n",
       "      <td>15.480</td>\n",
       "      <td>27.27</td>\n",
       "      <td>105.90</td>\n",
       "      <td>733.5</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.31710</td>\n",
       "      <td>0.36620</td>\n",
       "      <td>0.11050</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.08004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>18.460</td>\n",
       "      <td>18.52</td>\n",
       "      <td>121.10</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>0.09874</td>\n",
       "      <td>0.10530</td>\n",
       "      <td>0.13350</td>\n",
       "      <td>0.087950</td>\n",
       "      <td>0.2132</td>\n",
       "      <td>0.06022</td>\n",
       "      <td>...</td>\n",
       "      <td>22.930</td>\n",
       "      <td>27.68</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1603.0</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.20890</td>\n",
       "      <td>0.31570</td>\n",
       "      <td>0.16420</td>\n",
       "      <td>0.3695</td>\n",
       "      <td>0.08579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>27.220</td>\n",
       "      <td>21.87</td>\n",
       "      <td>182.10</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>0.10940</td>\n",
       "      <td>0.19140</td>\n",
       "      <td>0.28710</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.05770</td>\n",
       "      <td>...</td>\n",
       "      <td>33.120</td>\n",
       "      <td>32.85</td>\n",
       "      <td>220.80</td>\n",
       "      <td>3216.0</td>\n",
       "      <td>0.14720</td>\n",
       "      <td>0.40340</td>\n",
       "      <td>0.53400</td>\n",
       "      <td>0.26880</td>\n",
       "      <td>0.2856</td>\n",
       "      <td>0.08082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>17.010</td>\n",
       "      <td>20.26</td>\n",
       "      <td>109.70</td>\n",
       "      <td>904.3</td>\n",
       "      <td>0.08772</td>\n",
       "      <td>0.07304</td>\n",
       "      <td>0.06950</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.2026</td>\n",
       "      <td>0.05223</td>\n",
       "      <td>...</td>\n",
       "      <td>19.800</td>\n",
       "      <td>25.05</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>0.11110</td>\n",
       "      <td>0.14860</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.3275</td>\n",
       "      <td>0.06469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>12.940</td>\n",
       "      <td>16.17</td>\n",
       "      <td>83.18</td>\n",
       "      <td>507.6</td>\n",
       "      <td>0.09879</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>0.03296</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.06200</td>\n",
       "      <td>...</td>\n",
       "      <td>13.860</td>\n",
       "      <td>23.02</td>\n",
       "      <td>89.69</td>\n",
       "      <td>580.9</td>\n",
       "      <td>0.11720</td>\n",
       "      <td>0.19580</td>\n",
       "      <td>0.18100</td>\n",
       "      <td>0.08388</td>\n",
       "      <td>0.3297</td>\n",
       "      <td>0.07834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>11.410</td>\n",
       "      <td>10.82</td>\n",
       "      <td>73.34</td>\n",
       "      <td>403.3</td>\n",
       "      <td>0.09373</td>\n",
       "      <td>0.06685</td>\n",
       "      <td>0.03512</td>\n",
       "      <td>0.026230</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.06113</td>\n",
       "      <td>...</td>\n",
       "      <td>12.820</td>\n",
       "      <td>15.97</td>\n",
       "      <td>83.74</td>\n",
       "      <td>510.5</td>\n",
       "      <td>0.15480</td>\n",
       "      <td>0.23900</td>\n",
       "      <td>0.21020</td>\n",
       "      <td>0.08958</td>\n",
       "      <td>0.3016</td>\n",
       "      <td>0.08523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>13.820</td>\n",
       "      <td>24.49</td>\n",
       "      <td>92.33</td>\n",
       "      <td>595.9</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.16810</td>\n",
       "      <td>0.13570</td>\n",
       "      <td>0.067590</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.07237</td>\n",
       "      <td>...</td>\n",
       "      <td>16.010</td>\n",
       "      <td>32.94</td>\n",
       "      <td>106.00</td>\n",
       "      <td>788.0</td>\n",
       "      <td>0.17940</td>\n",
       "      <td>0.39660</td>\n",
       "      <td>0.33810</td>\n",
       "      <td>0.15210</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.11830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.02974</td>\n",
       "      <td>0.024430</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>14.250</td>\n",
       "      <td>22.15</td>\n",
       "      <td>96.42</td>\n",
       "      <td>645.7</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.20080</td>\n",
       "      <td>0.21350</td>\n",
       "      <td>0.086530</td>\n",
       "      <td>0.1949</td>\n",
       "      <td>0.07292</td>\n",
       "      <td>...</td>\n",
       "      <td>17.670</td>\n",
       "      <td>29.51</td>\n",
       "      <td>119.10</td>\n",
       "      <td>959.5</td>\n",
       "      <td>0.16400</td>\n",
       "      <td>0.62470</td>\n",
       "      <td>0.69220</td>\n",
       "      <td>0.17850</td>\n",
       "      <td>0.2844</td>\n",
       "      <td>0.11320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>11.710</td>\n",
       "      <td>15.45</td>\n",
       "      <td>75.03</td>\n",
       "      <td>420.3</td>\n",
       "      <td>0.11500</td>\n",
       "      <td>0.07281</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.2009</td>\n",
       "      <td>0.06506</td>\n",
       "      <td>...</td>\n",
       "      <td>13.060</td>\n",
       "      <td>18.16</td>\n",
       "      <td>84.16</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.14600</td>\n",
       "      <td>0.11150</td>\n",
       "      <td>0.10870</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.2765</td>\n",
       "      <td>0.07806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>13.210</td>\n",
       "      <td>25.25</td>\n",
       "      <td>84.10</td>\n",
       "      <td>537.9</td>\n",
       "      <td>0.08791</td>\n",
       "      <td>0.05205</td>\n",
       "      <td>0.02772</td>\n",
       "      <td>0.020680</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.05584</td>\n",
       "      <td>...</td>\n",
       "      <td>14.350</td>\n",
       "      <td>34.23</td>\n",
       "      <td>91.29</td>\n",
       "      <td>632.9</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.10630</td>\n",
       "      <td>0.13900</td>\n",
       "      <td>0.06005</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.06788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11.840</td>\n",
       "      <td>18.70</td>\n",
       "      <td>77.93</td>\n",
       "      <td>440.6</td>\n",
       "      <td>0.11090</td>\n",
       "      <td>0.15160</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.051820</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.07799</td>\n",
       "      <td>...</td>\n",
       "      <td>16.820</td>\n",
       "      <td>28.12</td>\n",
       "      <td>119.40</td>\n",
       "      <td>888.7</td>\n",
       "      <td>0.16370</td>\n",
       "      <td>0.57750</td>\n",
       "      <td>0.69560</td>\n",
       "      <td>0.15460</td>\n",
       "      <td>0.4761</td>\n",
       "      <td>0.14020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>10.290</td>\n",
       "      <td>27.61</td>\n",
       "      <td>65.67</td>\n",
       "      <td>321.4</td>\n",
       "      <td>0.09030</td>\n",
       "      <td>0.07658</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.027380</td>\n",
       "      <td>0.1593</td>\n",
       "      <td>0.06127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.840</td>\n",
       "      <td>34.91</td>\n",
       "      <td>69.57</td>\n",
       "      <td>357.6</td>\n",
       "      <td>0.13840</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.09127</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.08283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>10.570</td>\n",
       "      <td>18.32</td>\n",
       "      <td>66.82</td>\n",
       "      <td>340.9</td>\n",
       "      <td>0.08142</td>\n",
       "      <td>0.04462</td>\n",
       "      <td>0.01993</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.05768</td>\n",
       "      <td>...</td>\n",
       "      <td>10.940</td>\n",
       "      <td>23.31</td>\n",
       "      <td>69.35</td>\n",
       "      <td>366.3</td>\n",
       "      <td>0.09794</td>\n",
       "      <td>0.06542</td>\n",
       "      <td>0.03986</td>\n",
       "      <td>0.02222</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.06736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>17.910</td>\n",
       "      <td>21.02</td>\n",
       "      <td>124.40</td>\n",
       "      <td>994.0</td>\n",
       "      <td>0.12300</td>\n",
       "      <td>0.25760</td>\n",
       "      <td>0.31890</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>...</td>\n",
       "      <td>20.800</td>\n",
       "      <td>27.78</td>\n",
       "      <td>149.60</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>0.18730</td>\n",
       "      <td>0.59170</td>\n",
       "      <td>0.90340</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0.3245</td>\n",
       "      <td>0.11980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.450</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.080890</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.470</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.17910</td>\n",
       "      <td>0.52490</td>\n",
       "      <td>0.53550</td>\n",
       "      <td>0.17410</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>8.618</td>\n",
       "      <td>11.79</td>\n",
       "      <td>54.34</td>\n",
       "      <td>224.5</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.05272</td>\n",
       "      <td>0.02061</td>\n",
       "      <td>0.007799</td>\n",
       "      <td>0.1683</td>\n",
       "      <td>0.07187</td>\n",
       "      <td>...</td>\n",
       "      <td>9.507</td>\n",
       "      <td>15.40</td>\n",
       "      <td>59.90</td>\n",
       "      <td>274.9</td>\n",
       "      <td>0.17330</td>\n",
       "      <td>0.12390</td>\n",
       "      <td>0.11680</td>\n",
       "      <td>0.04419</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.09026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>12.650</td>\n",
       "      <td>18.17</td>\n",
       "      <td>82.69</td>\n",
       "      <td>485.6</td>\n",
       "      <td>0.10760</td>\n",
       "      <td>0.13340</td>\n",
       "      <td>0.08017</td>\n",
       "      <td>0.050740</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.06854</td>\n",
       "      <td>...</td>\n",
       "      <td>14.380</td>\n",
       "      <td>22.15</td>\n",
       "      <td>95.29</td>\n",
       "      <td>633.7</td>\n",
       "      <td>0.15330</td>\n",
       "      <td>0.38420</td>\n",
       "      <td>0.35820</td>\n",
       "      <td>0.14070</td>\n",
       "      <td>0.3230</td>\n",
       "      <td>0.10330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>11.370</td>\n",
       "      <td>18.89</td>\n",
       "      <td>72.17</td>\n",
       "      <td>396.0</td>\n",
       "      <td>0.08713</td>\n",
       "      <td>0.05008</td>\n",
       "      <td>0.02399</td>\n",
       "      <td>0.021730</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.05955</td>\n",
       "      <td>...</td>\n",
       "      <td>12.360</td>\n",
       "      <td>26.14</td>\n",
       "      <td>79.29</td>\n",
       "      <td>459.3</td>\n",
       "      <td>0.11180</td>\n",
       "      <td>0.09708</td>\n",
       "      <td>0.07529</td>\n",
       "      <td>0.06203</td>\n",
       "      <td>0.3267</td>\n",
       "      <td>0.06994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>12.060</td>\n",
       "      <td>18.90</td>\n",
       "      <td>76.66</td>\n",
       "      <td>445.3</td>\n",
       "      <td>0.08386</td>\n",
       "      <td>0.05794</td>\n",
       "      <td>0.00751</td>\n",
       "      <td>0.008488</td>\n",
       "      <td>0.1555</td>\n",
       "      <td>0.06048</td>\n",
       "      <td>...</td>\n",
       "      <td>13.640</td>\n",
       "      <td>27.06</td>\n",
       "      <td>86.54</td>\n",
       "      <td>562.6</td>\n",
       "      <td>0.12890</td>\n",
       "      <td>0.13520</td>\n",
       "      <td>0.04506</td>\n",
       "      <td>0.05093</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.08083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>11.670</td>\n",
       "      <td>20.02</td>\n",
       "      <td>75.21</td>\n",
       "      <td>416.2</td>\n",
       "      <td>0.10160</td>\n",
       "      <td>0.09453</td>\n",
       "      <td>0.04200</td>\n",
       "      <td>0.021570</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>0.06461</td>\n",
       "      <td>...</td>\n",
       "      <td>13.350</td>\n",
       "      <td>28.81</td>\n",
       "      <td>87.00</td>\n",
       "      <td>550.6</td>\n",
       "      <td>0.15500</td>\n",
       "      <td>0.29640</td>\n",
       "      <td>0.27580</td>\n",
       "      <td>0.08120</td>\n",
       "      <td>0.3206</td>\n",
       "      <td>0.08950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>11.600</td>\n",
       "      <td>24.49</td>\n",
       "      <td>74.23</td>\n",
       "      <td>417.2</td>\n",
       "      <td>0.07474</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01974</td>\n",
       "      <td>0.013130</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.05878</td>\n",
       "      <td>...</td>\n",
       "      <td>12.440</td>\n",
       "      <td>31.62</td>\n",
       "      <td>81.39</td>\n",
       "      <td>476.5</td>\n",
       "      <td>0.09545</td>\n",
       "      <td>0.13610</td>\n",
       "      <td>0.07239</td>\n",
       "      <td>0.04815</td>\n",
       "      <td>0.3244</td>\n",
       "      <td>0.06745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>10.030</td>\n",
       "      <td>21.28</td>\n",
       "      <td>63.19</td>\n",
       "      <td>307.3</td>\n",
       "      <td>0.08117</td>\n",
       "      <td>0.03912</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.06439</td>\n",
       "      <td>...</td>\n",
       "      <td>11.110</td>\n",
       "      <td>28.94</td>\n",
       "      <td>69.92</td>\n",
       "      <td>376.3</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.07094</td>\n",
       "      <td>0.01235</td>\n",
       "      <td>0.02579</td>\n",
       "      <td>0.2349</td>\n",
       "      <td>0.08061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>14.760</td>\n",
       "      <td>14.74</td>\n",
       "      <td>94.87</td>\n",
       "      <td>668.7</td>\n",
       "      <td>0.08875</td>\n",
       "      <td>0.07780</td>\n",
       "      <td>0.04608</td>\n",
       "      <td>0.035280</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.05912</td>\n",
       "      <td>...</td>\n",
       "      <td>17.270</td>\n",
       "      <td>17.93</td>\n",
       "      <td>114.20</td>\n",
       "      <td>880.8</td>\n",
       "      <td>0.12200</td>\n",
       "      <td>0.20090</td>\n",
       "      <td>0.21510</td>\n",
       "      <td>0.12510</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.08187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>14.400</td>\n",
       "      <td>26.99</td>\n",
       "      <td>92.25</td>\n",
       "      <td>646.1</td>\n",
       "      <td>0.06995</td>\n",
       "      <td>0.05223</td>\n",
       "      <td>0.03476</td>\n",
       "      <td>0.017370</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>0.05433</td>\n",
       "      <td>...</td>\n",
       "      <td>15.400</td>\n",
       "      <td>31.98</td>\n",
       "      <td>100.40</td>\n",
       "      <td>734.6</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.14600</td>\n",
       "      <td>0.14720</td>\n",
       "      <td>0.05563</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.06464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>14.970</td>\n",
       "      <td>19.76</td>\n",
       "      <td>95.50</td>\n",
       "      <td>690.2</td>\n",
       "      <td>0.08421</td>\n",
       "      <td>0.05352</td>\n",
       "      <td>0.01947</td>\n",
       "      <td>0.019390</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>0.05266</td>\n",
       "      <td>...</td>\n",
       "      <td>15.980</td>\n",
       "      <td>25.82</td>\n",
       "      <td>102.30</td>\n",
       "      <td>782.1</td>\n",
       "      <td>0.10450</td>\n",
       "      <td>0.09995</td>\n",
       "      <td>0.07750</td>\n",
       "      <td>0.05754</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.06085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>10.860</td>\n",
       "      <td>21.48</td>\n",
       "      <td>68.51</td>\n",
       "      <td>360.5</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.04227</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1661</td>\n",
       "      <td>0.05948</td>\n",
       "      <td>...</td>\n",
       "      <td>11.660</td>\n",
       "      <td>24.77</td>\n",
       "      <td>74.08</td>\n",
       "      <td>412.3</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>13.770</td>\n",
       "      <td>13.27</td>\n",
       "      <td>88.06</td>\n",
       "      <td>582.7</td>\n",
       "      <td>0.09198</td>\n",
       "      <td>0.06221</td>\n",
       "      <td>0.01063</td>\n",
       "      <td>0.019170</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.05912</td>\n",
       "      <td>...</td>\n",
       "      <td>14.670</td>\n",
       "      <td>16.93</td>\n",
       "      <td>94.17</td>\n",
       "      <td>661.1</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.03732</td>\n",
       "      <td>0.05802</td>\n",
       "      <td>0.2823</td>\n",
       "      <td>0.06794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>17.950</td>\n",
       "      <td>20.01</td>\n",
       "      <td>114.20</td>\n",
       "      <td>982.0</td>\n",
       "      <td>0.08402</td>\n",
       "      <td>0.06722</td>\n",
       "      <td>0.07293</td>\n",
       "      <td>0.055960</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.05025</td>\n",
       "      <td>...</td>\n",
       "      <td>20.580</td>\n",
       "      <td>27.83</td>\n",
       "      <td>129.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.12020</td>\n",
       "      <td>0.22490</td>\n",
       "      <td>0.11850</td>\n",
       "      <td>0.4882</td>\n",
       "      <td>0.06111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>15.460</td>\n",
       "      <td>11.89</td>\n",
       "      <td>102.50</td>\n",
       "      <td>736.9</td>\n",
       "      <td>0.12570</td>\n",
       "      <td>0.15550</td>\n",
       "      <td>0.20320</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.07069</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790</td>\n",
       "      <td>17.04</td>\n",
       "      <td>125.00</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.35830</td>\n",
       "      <td>0.58300</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.10100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.420</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.68690</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.31300</td>\n",
       "      <td>0.043750</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.08046</td>\n",
       "      <td>...</td>\n",
       "      <td>10.310</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>14.530</td>\n",
       "      <td>19.34</td>\n",
       "      <td>94.25</td>\n",
       "      <td>659.7</td>\n",
       "      <td>0.08388</td>\n",
       "      <td>0.07800</td>\n",
       "      <td>0.08817</td>\n",
       "      <td>0.029250</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.05746</td>\n",
       "      <td>...</td>\n",
       "      <td>16.300</td>\n",
       "      <td>28.39</td>\n",
       "      <td>108.10</td>\n",
       "      <td>830.5</td>\n",
       "      <td>0.10890</td>\n",
       "      <td>0.26490</td>\n",
       "      <td>0.37790</td>\n",
       "      <td>0.09594</td>\n",
       "      <td>0.2471</td>\n",
       "      <td>0.07463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>13.780</td>\n",
       "      <td>15.79</td>\n",
       "      <td>88.37</td>\n",
       "      <td>585.9</td>\n",
       "      <td>0.08817</td>\n",
       "      <td>0.06718</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>0.1405</td>\n",
       "      <td>0.05848</td>\n",
       "      <td>...</td>\n",
       "      <td>15.270</td>\n",
       "      <td>17.50</td>\n",
       "      <td>97.90</td>\n",
       "      <td>706.6</td>\n",
       "      <td>0.10720</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0.03517</td>\n",
       "      <td>0.03312</td>\n",
       "      <td>0.1859</td>\n",
       "      <td>0.06810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "421       14.690         13.98           98.22      656.1          0.10310   \n",
       "47        13.170         18.66           85.98      534.6          0.11580   \n",
       "292       12.950         16.02           83.14      513.7          0.10050   \n",
       "186       18.310         18.58          118.60     1041.0          0.08588   \n",
       "414       15.130         29.81           96.71      719.5          0.08320   \n",
       "132       16.160         21.54          106.20      809.8          0.10080   \n",
       "161       19.190         15.94          126.30     1157.0          0.08694   \n",
       "197       18.080         21.84          117.40     1024.0          0.07371   \n",
       "245       10.480         19.86           66.72      337.7          0.10700   \n",
       "453       14.530         13.98           93.86      644.2          0.10990   \n",
       "411       11.040         16.83           70.92      373.2          0.10770   \n",
       "214       14.190         23.81           92.87      610.7          0.09463   \n",
       "283       16.240         18.77          108.80      805.1          0.10660   \n",
       "107       12.360         18.54           79.01      466.7          0.08477   \n",
       "542       14.740         25.42           94.70      668.6          0.08275   \n",
       "518       12.880         18.22           84.45      493.1          0.12180   \n",
       "324       12.200         15.21           78.01      457.9          0.08673   \n",
       "488       11.680         16.17           75.49      420.5          0.11280   \n",
       "376       10.570         20.22           70.15      338.3          0.09073   \n",
       "237       20.480         21.46          132.50     1306.0          0.08355   \n",
       "362       12.760         18.84           81.87      496.6          0.09676   \n",
       "420       11.570         19.04           74.20      409.7          0.08546   \n",
       "451       19.590         25.00          127.70     1191.0          0.10320   \n",
       "519       12.750         16.70           82.51      493.8          0.11250   \n",
       "65        14.780         23.94           97.40      668.3          0.11720   \n",
       "242       11.300         18.19           73.93      389.4          0.09592   \n",
       "558       14.590         22.68           96.39      657.1          0.08473   \n",
       "85        18.460         18.52          121.10     1075.0          0.09874   \n",
       "180       27.220         21.87          182.10     2250.0          0.10940   \n",
       "207       17.010         20.26          109.70      904.3          0.08772   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "403       12.940         16.17           83.18      507.6          0.09879   \n",
       "120       11.410         10.82           73.34      403.3          0.09373   \n",
       "501       13.820         24.49           92.33      595.9          0.11620   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "62        14.250         22.15           96.42      645.7          0.10490   \n",
       "344       11.710         15.45           75.03      420.3          0.11500   \n",
       "457       13.210         25.25           84.10      537.9          0.08791   \n",
       "31        11.840         18.70           77.93      440.6          0.11090   \n",
       "555       10.290         27.61           65.67      321.4          0.09030   \n",
       "443       10.570         18.32           66.82      340.9          0.08142   \n",
       "400       17.910         21.02          124.40      994.0          0.12300   \n",
       "5         12.450         15.70           82.57      477.1          0.12780   \n",
       "59         8.618         11.79           54.34      224.5          0.09752   \n",
       "496       12.650         18.17           82.69      485.6          0.10760   \n",
       "289       11.370         18.89           72.17      396.0          0.08713   \n",
       "346       12.060         18.90           76.66      445.3          0.08386   \n",
       "531       11.670         20.02           75.21      416.2          0.10160   \n",
       "305       11.600         24.49           74.23      417.2          0.07474   \n",
       "425       10.030         21.28           63.19      307.3          0.08117   \n",
       "347       14.760         14.74           94.87      668.7          0.08875   \n",
       "462       14.400         26.99           92.25      646.1          0.06995   \n",
       "165       14.970         19.76           95.50      690.2          0.08421   \n",
       "550       10.860         21.48           68.51      360.5          0.07431   \n",
       "295       13.770         13.27           88.06      582.7          0.09198   \n",
       "119       17.950         20.01          114.20      982.0          0.08402   \n",
       "172       15.460         11.89          102.50      736.9          0.12570   \n",
       "3         11.420         20.38           77.58      386.1          0.14250   \n",
       "68         9.029         17.33           58.79      250.5          0.10660   \n",
       "448       14.530         19.34           94.25      659.7          0.08388   \n",
       "442       13.780         15.79           88.37      585.9          0.08817   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "421           0.18360         0.14500             0.063000         0.2086   \n",
       "47            0.12310         0.12260             0.073400         0.2128   \n",
       "292           0.07943         0.06155             0.033700         0.1730   \n",
       "186           0.08468         0.08169             0.058140         0.1621   \n",
       "414           0.04605         0.04686             0.027390         0.1852   \n",
       "132           0.12840         0.10430             0.056130         0.2160   \n",
       "161           0.11850         0.11930             0.096670         0.1741   \n",
       "197           0.08642         0.11030             0.057780         0.1770   \n",
       "245           0.05971         0.04831             0.030700         0.1737   \n",
       "453           0.09242         0.06895             0.064950         0.1650   \n",
       "411           0.07804         0.03046             0.024800         0.1714   \n",
       "214           0.13060         0.11150             0.064620         0.2235   \n",
       "283           0.18020         0.19480             0.090520         0.1876   \n",
       "107           0.06815         0.02643             0.019210         0.1602   \n",
       "542           0.07214         0.04105             0.030270         0.1840   \n",
       "518           0.16610         0.04825             0.053030         0.1709   \n",
       "324           0.06545         0.01994             0.016920         0.1638   \n",
       "488           0.09263         0.04279             0.031320         0.1853   \n",
       "376           0.16600         0.22800             0.059410         0.2188   \n",
       "237           0.08348         0.09042             0.060220         0.1467   \n",
       "362           0.07952         0.02688             0.017810         0.1759   \n",
       "420           0.07722         0.05485             0.014280         0.2031   \n",
       "451           0.09871         0.16550             0.090630         0.1663   \n",
       "519           0.11170         0.03880             0.029950         0.2120   \n",
       "65            0.14790         0.12670             0.090290         0.1953   \n",
       "242           0.13250         0.15480             0.028540         0.2054   \n",
       "558           0.13300         0.10290             0.037360         0.1454   \n",
       "85            0.10530         0.13350             0.087950         0.2132   \n",
       "180           0.19140         0.28710             0.187800         0.1800   \n",
       "207           0.07304         0.06950             0.053900         0.2026   \n",
       "..                ...             ...                  ...            ...   \n",
       "403           0.08836         0.03296             0.023900         0.1735   \n",
       "120           0.06685         0.03512             0.026230         0.1667   \n",
       "501           0.16810         0.13570             0.067590         0.2275   \n",
       "545           0.06747         0.02974             0.024430         0.1664   \n",
       "62            0.20080         0.21350             0.086530         0.1949   \n",
       "344           0.07281         0.04006             0.032500         0.2009   \n",
       "457           0.05205         0.02772             0.020680         0.1619   \n",
       "31            0.15160         0.12180             0.051820         0.2301   \n",
       "555           0.07658         0.05999             0.027380         0.1593   \n",
       "443           0.04462         0.01993             0.011110         0.2372   \n",
       "400           0.25760         0.31890             0.119800         0.2113   \n",
       "5             0.17000         0.15780             0.080890         0.2087   \n",
       "59            0.05272         0.02061             0.007799         0.1683   \n",
       "496           0.13340         0.08017             0.050740         0.1641   \n",
       "289           0.05008         0.02399             0.021730         0.2013   \n",
       "346           0.05794         0.00751             0.008488         0.1555   \n",
       "531           0.09453         0.04200             0.021570         0.1859   \n",
       "305           0.05688         0.01974             0.013130         0.1935   \n",
       "425           0.03912         0.00247             0.005159         0.1630   \n",
       "347           0.07780         0.04608             0.035280         0.1521   \n",
       "462           0.05223         0.03476             0.017370         0.1707   \n",
       "165           0.05352         0.01947             0.019390         0.1515   \n",
       "550           0.04227         0.00000             0.000000         0.1661   \n",
       "295           0.06221         0.01063             0.019170         0.1592   \n",
       "119           0.06722         0.07293             0.055960         0.2129   \n",
       "172           0.15550         0.20320             0.109700         0.1966   \n",
       "3             0.28390         0.24140             0.105200         0.2597   \n",
       "68            0.14130         0.31300             0.043750         0.2111   \n",
       "448           0.07800         0.08817             0.029250         0.1473   \n",
       "442           0.06718         0.01055             0.009937         0.1405   \n",
       "\n",
       "     mean fractal dimension           ...             worst radius  \\\n",
       "421                 0.07406           ...                   16.460   \n",
       "47                  0.06777           ...                   15.670   \n",
       "292                 0.06470           ...                   13.740   \n",
       "186                 0.05425           ...                   21.310   \n",
       "414                 0.05294           ...                   17.260   \n",
       "132                 0.05891           ...                   19.470   \n",
       "161                 0.05176           ...                   22.030   \n",
       "197                 0.05340           ...                   19.760   \n",
       "245                 0.06440           ...                   11.480   \n",
       "453                 0.06121           ...                   15.800   \n",
       "411                 0.06340           ...                   12.410   \n",
       "214                 0.06433           ...                   16.860   \n",
       "283                 0.06684           ...                   18.550   \n",
       "107                 0.06066           ...                   13.290   \n",
       "542                 0.05680           ...                   16.510   \n",
       "518                 0.07253           ...                   15.050   \n",
       "324                 0.06129           ...                   13.750   \n",
       "488                 0.06401           ...                   13.320   \n",
       "376                 0.08450           ...                   10.850   \n",
       "237                 0.05177           ...                   24.220   \n",
       "362                 0.06183           ...                   13.750   \n",
       "420                 0.06267           ...                   13.070   \n",
       "451                 0.05391           ...                   21.440   \n",
       "519                 0.06623           ...                   14.450   \n",
       "65                  0.06654           ...                   17.310   \n",
       "242                 0.07669           ...                   12.580   \n",
       "558                 0.06147           ...                   15.480   \n",
       "85                  0.06022           ...                   22.930   \n",
       "180                 0.05770           ...                   33.120   \n",
       "207                 0.05223           ...                   19.800   \n",
       "..                      ...           ...                      ...   \n",
       "403                 0.06200           ...                   13.860   \n",
       "120                 0.06113           ...                   12.820   \n",
       "501                 0.07237           ...                   16.010   \n",
       "545                 0.05801           ...                   15.350   \n",
       "62                  0.07292           ...                   17.670   \n",
       "344                 0.06506           ...                   13.060   \n",
       "457                 0.05584           ...                   14.350   \n",
       "31                  0.07799           ...                   16.820   \n",
       "555                 0.06127           ...                   10.840   \n",
       "443                 0.05768           ...                   10.940   \n",
       "400                 0.07115           ...                   20.800   \n",
       "5                   0.07613           ...                   15.470   \n",
       "59                  0.07187           ...                    9.507   \n",
       "496                 0.06854           ...                   14.380   \n",
       "289                 0.05955           ...                   12.360   \n",
       "346                 0.06048           ...                   13.640   \n",
       "531                 0.06461           ...                   13.350   \n",
       "305                 0.05878           ...                   12.440   \n",
       "425                 0.06439           ...                   11.110   \n",
       "347                 0.05912           ...                   17.270   \n",
       "462                 0.05433           ...                   15.400   \n",
       "165                 0.05266           ...                   15.980   \n",
       "550                 0.05948           ...                   11.660   \n",
       "295                 0.05912           ...                   14.670   \n",
       "119                 0.05025           ...                   20.580   \n",
       "172                 0.07069           ...                   18.790   \n",
       "3                   0.09744           ...                   14.910   \n",
       "68                  0.08046           ...                   10.310   \n",
       "448                 0.05746           ...                   16.300   \n",
       "442                 0.05848           ...                   15.270   \n",
       "\n",
       "     worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "421          18.34           114.10       809.2           0.13120   \n",
       "47           27.95           102.80       759.4           0.17860   \n",
       "292          19.93            88.81       585.4           0.14830   \n",
       "186          26.36           139.20      1410.0           0.12340   \n",
       "414          36.91           110.10       931.4           0.11480   \n",
       "132          31.68           129.70      1175.0           0.13950   \n",
       "161          17.81           146.60      1495.0           0.11240   \n",
       "197          24.70           129.10      1228.0           0.08822   \n",
       "245          29.46            73.68       402.8           0.15150   \n",
       "453          16.93           103.10       749.9           0.13470   \n",
       "411          26.44            79.93       471.4           0.13690   \n",
       "214          34.85           115.00       811.3           0.15590   \n",
       "283          25.09           126.90      1031.0           0.13650   \n",
       "107          27.49            85.56       544.1           0.11840   \n",
       "542          32.29           107.40       826.4           0.10600   \n",
       "518          24.37            99.31       674.7           0.14560   \n",
       "324          21.38            91.11       583.1           0.12560   \n",
       "488          21.59            86.57       549.8           0.15260   \n",
       "376          22.82            76.51       351.9           0.11430   \n",
       "237          26.17           161.70      1750.0           0.12280   \n",
       "362          25.99            87.82       579.7           0.12980   \n",
       "420          26.98            86.43       520.5           0.12490   \n",
       "451          30.96           139.80      1421.0           0.15280   \n",
       "519          21.74            93.63       624.1           0.14750   \n",
       "65           33.39           114.60       925.1           0.16480   \n",
       "242          27.96            87.16       472.9           0.13470   \n",
       "558          27.27           105.90       733.5           0.10260   \n",
       "85           27.68           152.20      1603.0           0.13980   \n",
       "180          32.85           220.80      3216.0           0.14720   \n",
       "207          25.05           130.00      1210.0           0.11110   \n",
       "..             ...              ...         ...               ...   \n",
       "403          23.02            89.69       580.9           0.11720   \n",
       "120          15.97            83.74       510.5           0.15480   \n",
       "501          32.94           106.00       788.0           0.17940   \n",
       "545          29.09            97.58       729.8           0.12160   \n",
       "62           29.51           119.10       959.5           0.16400   \n",
       "344          18.16            84.16       516.4           0.14600   \n",
       "457          34.23            91.29       632.9           0.12890   \n",
       "31           28.12           119.40       888.7           0.16370   \n",
       "555          34.91            69.57       357.6           0.13840   \n",
       "443          23.31            69.35       366.3           0.09794   \n",
       "400          27.78           149.60      1304.0           0.18730   \n",
       "5            23.75           103.40       741.6           0.17910   \n",
       "59           15.40            59.90       274.9           0.17330   \n",
       "496          22.15            95.29       633.7           0.15330   \n",
       "289          26.14            79.29       459.3           0.11180   \n",
       "346          27.06            86.54       562.6           0.12890   \n",
       "531          28.81            87.00       550.6           0.15500   \n",
       "305          31.62            81.39       476.5           0.09545   \n",
       "425          28.94            69.92       376.3           0.11260   \n",
       "347          17.93           114.20       880.8           0.12200   \n",
       "462          31.98           100.40       734.6           0.10170   \n",
       "165          25.82           102.30       782.1           0.10450   \n",
       "550          24.77            74.08       412.3           0.10010   \n",
       "295          16.93            94.17       661.1           0.11700   \n",
       "119          27.83           129.20      1261.0           0.10720   \n",
       "172          17.04           125.00      1102.0           0.15310   \n",
       "3            26.50            98.87       567.7           0.20980   \n",
       "68           22.65            65.50       324.7           0.14820   \n",
       "448          28.39           108.10       830.5           0.10890   \n",
       "442          17.50            97.90       706.6           0.10720   \n",
       "\n",
       "     worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "421            0.36350          0.32190               0.11080          0.2827   \n",
       "47             0.41660          0.50060               0.20880          0.3900   \n",
       "292            0.20680          0.22410               0.10560          0.3380   \n",
       "186            0.24450          0.35380               0.15710          0.3206   \n",
       "414            0.09866          0.15470               0.06575          0.3233   \n",
       "132            0.30550          0.29920               0.13120          0.3480   \n",
       "161            0.20160          0.22640               0.17770          0.2443   \n",
       "197            0.19630          0.25350               0.09181          0.2369   \n",
       "245            0.10260          0.11810               0.06736          0.2883   \n",
       "453            0.14780          0.13730               0.10690          0.2606   \n",
       "411            0.14820          0.10670               0.07431          0.2998   \n",
       "214            0.40590          0.37440               0.17720          0.4724   \n",
       "283            0.47060          0.50260               0.17320          0.2770   \n",
       "107            0.19630          0.19370               0.08442          0.2983   \n",
       "542            0.13760          0.16110               0.10950          0.2722   \n",
       "518            0.29610          0.12460               0.10960          0.2582   \n",
       "324            0.19280          0.11670               0.05556          0.2661   \n",
       "488            0.14770          0.14900               0.09815          0.2804   \n",
       "376            0.36190          0.60300               0.14650          0.2597   \n",
       "237            0.23110          0.31580               0.14450          0.2238   \n",
       "362            0.18390          0.12550               0.08312          0.2744   \n",
       "420            0.19370          0.25600               0.06664          0.3035   \n",
       "451            0.18450          0.39770               0.14660          0.2293   \n",
       "519            0.19790          0.14230               0.08045          0.3071   \n",
       "65             0.34160          0.30240               0.16140          0.3321   \n",
       "242            0.48480          0.74360               0.12180          0.3308   \n",
       "558            0.31710          0.36620               0.11050          0.2258   \n",
       "85             0.20890          0.31570               0.16420          0.3695   \n",
       "180            0.40340          0.53400               0.26880          0.2856   \n",
       "207            0.14860          0.19320               0.10960          0.3275   \n",
       "..                 ...              ...                   ...             ...   \n",
       "403            0.19580          0.18100               0.08388          0.3297   \n",
       "120            0.23900          0.21020               0.08958          0.3016   \n",
       "501            0.39660          0.33810               0.15210          0.3651   \n",
       "545            0.15170          0.10490               0.07174          0.2642   \n",
       "62             0.62470          0.69220               0.17850          0.2844   \n",
       "344            0.11150          0.10870               0.07864          0.2765   \n",
       "457            0.10630          0.13900               0.06005          0.2444   \n",
       "31             0.57750          0.69560               0.15460          0.4761   \n",
       "555            0.17100          0.20000               0.09127          0.2226   \n",
       "443            0.06542          0.03986               0.02222          0.2699   \n",
       "400            0.59170          0.90340               0.19640          0.3245   \n",
       "5              0.52490          0.53550               0.17410          0.3985   \n",
       "59             0.12390          0.11680               0.04419          0.3220   \n",
       "496            0.38420          0.35820               0.14070          0.3230   \n",
       "289            0.09708          0.07529               0.06203          0.3267   \n",
       "346            0.13520          0.04506               0.05093          0.2880   \n",
       "531            0.29640          0.27580               0.08120          0.3206   \n",
       "305            0.13610          0.07239               0.04815          0.3244   \n",
       "425            0.07094          0.01235               0.02579          0.2349   \n",
       "347            0.20090          0.21510               0.12510          0.3109   \n",
       "462            0.14600          0.14720               0.05563          0.2345   \n",
       "165            0.09995          0.07750               0.05754          0.2646   \n",
       "550            0.07348          0.00000               0.00000          0.2458   \n",
       "295            0.10720          0.03732               0.05802          0.2823   \n",
       "119            0.12020          0.22490               0.11850          0.4882   \n",
       "172            0.35830          0.58300               0.18270          0.3216   \n",
       "3              0.86630          0.68690               0.25750          0.6638   \n",
       "68             0.43650          1.25200               0.17500          0.4228   \n",
       "448            0.26490          0.37790               0.09594          0.2471   \n",
       "442            0.10710          0.03517               0.03312          0.1859   \n",
       "\n",
       "     worst fractal dimension  \n",
       "421                  0.09208  \n",
       "47                   0.11790  \n",
       "292                  0.09584  \n",
       "186                  0.06938  \n",
       "414                  0.06165  \n",
       "132                  0.07619  \n",
       "161                  0.06251  \n",
       "197                  0.06558  \n",
       "245                  0.07748  \n",
       "453                  0.07810  \n",
       "411                  0.07881  \n",
       "214                  0.10260  \n",
       "283                  0.10630  \n",
       "107                  0.07185  \n",
       "542                  0.06956  \n",
       "518                  0.08893  \n",
       "324                  0.07961  \n",
       "488                  0.08024  \n",
       "376                  0.12000  \n",
       "237                  0.07127  \n",
       "362                  0.07238  \n",
       "420                  0.08284  \n",
       "451                  0.06091  \n",
       "519                  0.08557  \n",
       "65                   0.08911  \n",
       "242                  0.12970  \n",
       "558                  0.08004  \n",
       "85                   0.08579  \n",
       "180                  0.08082  \n",
       "207                  0.06469  \n",
       "..                       ...  \n",
       "403                  0.07834  \n",
       "120                  0.08523  \n",
       "501                  0.11830  \n",
       "545                  0.06953  \n",
       "62                   0.11320  \n",
       "344                  0.07806  \n",
       "457                  0.06788  \n",
       "31                   0.14020  \n",
       "555                  0.08283  \n",
       "443                  0.06736  \n",
       "400                  0.11980  \n",
       "5                    0.12440  \n",
       "59                   0.09026  \n",
       "496                  0.10330  \n",
       "289                  0.06994  \n",
       "346                  0.08083  \n",
       "531                  0.08950  \n",
       "305                  0.06745  \n",
       "425                  0.08061  \n",
       "347                  0.08187  \n",
       "462                  0.06464  \n",
       "165                  0.06085  \n",
       "550                  0.06592  \n",
       "295                  0.06794  \n",
       "119                  0.06111  \n",
       "172                  0.10100  \n",
       "3                    0.17300  \n",
       "68                   0.11750  \n",
       "448                  0.07463  \n",
       "442                  0.06810  \n",
       "\n",
       "[114 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "load = load_breast_cancer()\n",
    "cancer = pd.DataFrame(load.data, columns=load.feature_names)\n",
    "target = pd.DataFrame(load.target, columns =['target'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(cancer, target, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "X_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_missing = [col for col in cancer.columns if (cancer[col].isnull().any())]\n",
    "cols_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train datasets\n",
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- worst perimeter <= 106.05\n",
      "|   |--- worst concave points <= 0.16\n",
      "|   |   |--- worst concave points <= 0.14\n",
      "|   |   |   |--- area error <= 48.98\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- area error >  48.98\n",
      "|   |   |   |   |--- worst radius <= 13.55\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- worst radius >  13.55\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |--- worst concave points >  0.14\n",
      "|   |   |   |--- worst texture <= 29.45\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- worst texture >  29.45\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |--- worst concave points >  0.16\n",
      "|   |   |--- worst texture <= 24.78\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- worst texture >  24.78\n",
      "|   |   |   |--- class: 0\n",
      "|--- worst perimeter >  106.05\n",
      "|   |--- worst texture <= 20.65\n",
      "|   |   |--- worst perimeter <= 116.80\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- worst perimeter >  116.80\n",
      "|   |   |   |--- mean smoothness <= 0.08\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- mean smoothness >  0.08\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |--- worst texture >  20.65\n",
      "|   |   |--- mean concave points <= 0.05\n",
      "|   |   |   |--- concave points error <= 0.01\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- concave points error >  0.01\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- mean concave points >  0.05\n",
      "|   |   |   |--- mean smoothness <= 0.08\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- mean smoothness >  0.08\n",
      "|   |   |   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "tree = export_text(classifier, feature_names = list(cancer.columns))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give prediction to training data so we can see how good the model is. Compared to true value of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac {Number of Correct predictions}{Total number of predictions made}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train_score = accuracy_score(y_train, predict_train)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc_train_score)\n",
    "\n",
    "acc_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 score Metrics. The metrics used based on this confusion matrix. F1 Score is the **_Harmonic Mean_** between precision and recall.\n",
    "<!-- ![Alt Text](image path \"title\") -->\n",
    "![Alt Text](https://miro.medium.com/max/700/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg \"Confusion Matrix\")\n",
    "\n",
    "$$\n",
    "Precision = \\frac {TruePositives}{TruePositives+FalsePositives}\n",
    "$$ <br>\n",
    "$$\n",
    "Recall = \\frac {TruePositives}{TruePositives+FalseNegatives}\n",
    "$$ <br>\n",
    "$$\n",
    "F1 = 2*(\\frac{1}{\\frac {1}{Precision} + \\frac {1}{Recall}}) = 2*(\\frac {TruePositives}{2TruePositives+FalsePositives+FalseNegatives})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train_score = f1_score(y_train, predict_train)\n",
    "\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1_train_score)\n",
    "\n",
    "f1_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_valid = classifier.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid_score = accuracy_score(y_valid, predict_valid)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc_valid_score)\n",
    "\n",
    "acc_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594594594594595"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid_score = f1_score(y_valid, predict_valid)\n",
    "\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1_valid_score)\n",
    "\n",
    "f1_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id3Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worst perimeter <=105.15\n",
      "|   worst concave points <=0.14\n",
      "|   |   radius error <=0.64: 1 (249) \n",
      "|   |   radius error >0.64\n",
      "|   |   |   mean radius <=12.27: 0 (1) \n",
      "|   |   |   mean radius >12.27: 1 (2) \n",
      "|   worst concave points >0.14\n",
      "|   |   worst texture <=25.94: 1 (6) \n",
      "|   |   worst texture >25.94\n",
      "|   |   |   mean compactness <=0.12\n",
      "|   |   |   |   mean radius <=14.06: 1 (2) \n",
      "|   |   |   |   mean radius >14.06: 0 (1) \n",
      "|   |   |   mean compactness >0.12: 0 (5) \n",
      "worst perimeter >105.15\n",
      "|   worst concave points <=0.15\n",
      "|   |   worst texture <=19.91: 1 (13) \n",
      "|   |   worst texture >19.91\n",
      "|   |   |   worst radius <=16.80\n",
      "|   |   |   |   mean smoothness <=0.09: 1 (8) \n",
      "|   |   |   |   mean smoothness >0.09\n",
      "|   |   |   |   |   smoothness error <=0.00: 1 (2) \n",
      "|   |   |   |   |   smoothness error >0.00: 0 (5) \n",
      "|   |   |   worst radius >16.80\n",
      "|   |   |   |   worst concavity <=0.21\n",
      "|   |   |   |   |   mean texture <=21.26: 1 (2) \n",
      "|   |   |   |   |   mean texture >21.26: 0 (2) \n",
      "|   |   |   |   worst concavity >0.21: 0 (21) \n",
      "|   worst concave points >0.15\n",
      "|   |   mean texture <=15.35\n",
      "|   |   |   mean radius <=14.89: 1 (1) \n",
      "|   |   |   mean radius >14.89: 0 (3) \n",
      "|   |   mean texture >15.35: 0 (132) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "from id3 import Id3Estimator, export\n",
    "\n",
    "id3_model1 = Id3Estimator()\n",
    "\n",
    "id3_model1.fit(X_train, y_train.values.ravel()) # make it numpy 1D array, not a df\n",
    "\n",
    "# export text\n",
    "tree_text1 = export.export_text(id3_model1.tree_, feature_names=cancer.columns)\n",
    "\n",
    "print(tree_text1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "predict_train1 = id3_model1.predict(X_train)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_train1, y_train))\n",
    "print(\"F1 score: \", f1_score(predict_train1, y_train))\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(accuracy_score(predict_train1, y_train))\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1_score(predict_train1, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to test data with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9385964912280702\n",
      "F1 score:  0.953020134228188\n"
     ]
    }
   ],
   "source": [
    "# Model Prediction\n",
    "predict_test1 = id3_model1.predict(X_valid)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_test1,y_valid))\n",
    "print(\"F1 score: \", f1_score(predict_test1,y_valid))\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(accuracy_score(predict_test1,y_valid))\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1_score(predict_test1,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine how many clusters\n",
    "n_clust = y_train['target'].nunique()\n",
    "\n",
    "kmeans_model_1 = KMeans(n_clusters=n_clust,random_state=1)\n",
    "kmeans_model_1.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the data train and data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 1 0 1 0 0]\n",
      "     target\n",
      "408       0\n",
      "4         0\n",
      "307       1\n",
      "386       1\n",
      "404       1\n",
      "434       1\n",
      "19        1\n",
      "517       0\n",
      "535       0\n",
      "445       1\n",
      "554       1\n",
      "236       0\n",
      "117       0\n",
      "157       1\n",
      "162       0\n",
      "78        0\n",
      "409       1\n",
      "484       1\n",
      "334       1\n",
      "42        0\n",
      "173       1\n",
      "223       0\n",
      "201       0\n",
      "133       1\n",
      "232       1\n",
      "413       1\n",
      "514       0\n",
      "244       0\n",
      "415       1\n",
      "562       0\n",
      "..      ...\n",
      "264       0\n",
      "209       1\n",
      "316       1\n",
      "513       1\n",
      "313       1\n",
      "534       1\n",
      "319       1\n",
      "7         0\n",
      "393       0\n",
      "141       0\n",
      "86        0\n",
      "478       1\n",
      "503       0\n",
      "215       0\n",
      "398       1\n",
      "490       1\n",
      "252       0\n",
      "468       0\n",
      "357       1\n",
      "254       0\n",
      "276       1\n",
      "178       1\n",
      "281       1\n",
      "390       1\n",
      "508       1\n",
      "129       0\n",
      "144       1\n",
      "72        0\n",
      "235       1\n",
      "37        1\n",
      "\n",
      "[455 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "kmeans_predict_train = kmeans_model_1.predict(X_train)\n",
    "kmeans_predict_test = kmeans_model_1.predict(X_valid)\n",
    "print(kmeans_predict_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to switch the value from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def switch(x: int) -> int:\n",
    "    if (x==0):\n",
    "        return x+1\n",
    "    elif (x==1):\n",
    "        return x-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.15164835164835164\n",
      "First clusterization F1: 0.005154639175257732\n",
      "Second clusterization accuracy: 0.8483516483516483\n",
      "Second clusterization F1: 0.891679748822606\n"
     ]
    }
   ],
   "source": [
    "# Find out each cluster belong with which class\n",
    "\n",
    "# First clusterization using first prediction (k_means_train)\n",
    "# Calculate the accuracy and F1\n",
    "acc_train = accuracy_score(y_train, kmeans_predict_train)\n",
    "f1_train = f1_score(y_train, kmeans_predict_train)\n",
    "print(f\"First clusterization accuracy: {acc_train}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_train = pd.Series(kmeans_predict_train)\n",
    "fix = switched_train.apply(switch)\n",
    "acc_train1= accuracy_score(y_train, fix)\n",
    "f1_train2= f1_score(y_train, fix)\n",
    "print(f\"Second clusterization accuracy: {acc_train1}\")\n",
    "print(f\"Second clusterization F1: {f1_train2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert to dict\n",
    "data['acc_train'].append(acc_train1)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance can be seen by the greatest metrics score. So the cluster 0 belongs to class of 1, cluster 1 belongs to class of 0. Now, for evaluation with validation data (datatest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.19298245614035087\n",
      "First clusterization F1: 0.005154639175257732\n",
      "Second clusterization accuracy: 0.8070175438596491\n",
      "Second clusterization F1: 0.8674698795180723\n"
     ]
    }
   ],
   "source": [
    "# First clusterization\n",
    "acc_valid = accuracy_score(y_valid, kmeans_predict_test)\n",
    "f1_valid = f1_score(y_valid, kmeans_predict_test)\n",
    "print(f\"First clusterization accuracy: {acc_valid}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_valid = pd.Series(kmeans_predict_test).apply(switch)\n",
    "acc_valid_switch = accuracy_score(y_valid, switched_valid)\n",
    "f1_valid_switch = f1_score(y_valid, switched_valid)\n",
    "print(f\"Second clusterization accuracy: {acc_valid_switch}\")\n",
    "print(f\"Second clusterization F1: {f1_valid_switch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert to dict\n",
    "data['acc_valid'].append(acc_valid_switch)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1_valid_switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as when we evaluate with training data, the second clusterization is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2500, random_state=1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model_1 = LogisticRegression(max_iter=2500, random_state=1)\n",
    "\n",
    "model_1.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data with only 2500 maximum iteration\n",
      "Accuracy:  0.9604395604395605\n",
      "F1 score:  0.9685314685314685\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_1.predict(X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Training Data with only 2500 maximum iteration\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data with only 2500 maximum iteration\n",
      "Accuracy:  0.9473684210526315\n",
      "F1 score:  0.9594594594594595\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_1.predict(X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Test Data with only 2500 maximum iteration\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the dataset (using Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_valid = scaler.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling the scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2500, random_state=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = LogisticRegression(max_iter=2500, random_state=1)\n",
    "model_2.fit(scaled_X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to train data with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data with using max 2500 iteration and StandardScaler\n",
      "Accuracy:  0.9912087912087912\n",
      "F1 score:  0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_2.predict(scaled_X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Training Data with using max 2500 iteration and StandardScaler\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to test data with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data with using max 2500 iteration and StandardScaler\n",
      "Accuracy:  0.9736842105263158\n",
      "F1 score:  0.9793103448275863\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_2.predict(scaled_X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Test Data with using max 2500 iteration and StandardScaler\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data (Using MinMax Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=2500, random_state=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_valid = scaler.fit_transform(X_valid)\n",
    "\n",
    "model_3 = LogisticRegression(max_iter=2500, random_state=1)\n",
    "model_3.fit(scaled_X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data with using max 2500 iteration and MinMaxScaler\n",
      "Accuracy:  0.9626373626373627\n",
      "F1 score:  0.9709401709401708\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_3.predict(scaled_X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Training Data with using max 2500 iteration and MinMaxScaler\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data with using max 2500 iteration and MinMaxScaler\n",
      "Accuracy:  0.9736842105263158\n",
      "F1 score:  0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_3.predict(scaled_X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Test Data with using max 2500 iteration and MinMaxScaler\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "    \n",
    "This data is better with MinMaxScaler because the metrics of the validation dataset bigger than the metrics of the training dataset, even this is a unusual condition. Possible answer: the data was divided into two imbalance dataset, so the training data harder to solve, and the validation data easier to solve. So, we still concluded that with MinMaxScaler is better, because it proves that model is not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=700, random_state=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_1 = MLPClassifier(max_iter = 700, random_state=1)\n",
    "\n",
    "model_1.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9428571428571428\n",
      "F1 score:  0.9559322033898305\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_1.predict(X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9473684210526315\n",
      "F1 score:  0.9594594594594595\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_1.predict(X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=700, random_state=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_valid = scaler.fit_transform(X_valid)\n",
    "\n",
    "model_2 = MLPClassifier(max_iter = 700, random_state=1)\n",
    "\n",
    "model_2.fit(scaled_X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_2.predict(scaled_X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9649122807017544\n",
      "F1 score:  0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_2.predict(scaled_X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the traning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=700, random_state=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_valid = scaler.fit_transform(X_valid)\n",
    "\n",
    "model_3 = MLPClassifier(max_iter = 700, random_state=1)\n",
    "model_3.fit(scaled_X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9934065934065934\n",
      "F1 score:  0.9947643979057591\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_3.predict(scaled_X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9035087719298246\n",
      "F1 score:  0.9172932330827067\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_3.predict(scaled_X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the SVM (Support Vector Machines) constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the traning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, y_train.values.ravel())\n",
    "# .values will give the values in an array (shape: (n,1)\n",
    "# .ravel will convert that array shape to (n, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_svm_breast = svm.predict(X_train)\n",
    "pred_val_svm_breast = svm.predict(X_valid)\n",
    "pred_val_svm_breast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate data train prediction with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9230769230769231\n",
      "F1 :  0.9405772495755519\n"
     ]
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_train, pred_train_svm_breast)\n",
    "f1 = f1_score(y_train, pred_train_svm_breast)\n",
    "print(\"Accuracy : \", acc_score)\n",
    "print(\"F1 : \",f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_train'].append(acc_score)\n",
    "# insert to dict\n",
    "data['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate data test prediction with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9035087719298246\n",
      "F1 :  0.9290322580645161\n"
     ]
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_valid, pred_val_svm_breast)\n",
    "f1 = f1_score(y_valid, pred_val_svm_breast)\n",
    "print(\"Accuracy : \", acc_score)\n",
    "print(\"F1 : \",f1)\n",
    "\n",
    "# insert to dict\n",
    "data['acc_valid'].append(acc_score)\n",
    "# insert to dict\n",
    "data['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    " breast_cancer = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play-Tennis Dataset\n",
    "\n",
    "Load the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise data of lists. \n",
    "data_tennis = {'algo':['DecisionTreeClassifier', 'Id3Estimator', 'KMeans','Logreg','Logreg_StdScaler', 'Logreg_MinMaxScaler','NN',\n",
    "               'NN_StdScaler','NN_MinMaxScaler','SVM'], \n",
    "        'acc_train':[],\n",
    "        'acc_valid':[],\n",
    "        'f1_train':[],\n",
    "        'f1_valid':[]\n",
    "       } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  play\n",
       "3  yes\n",
       "7   no\n",
       "6  yes"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis = pd.read_csv('datasets/PlayTennis.csv')\n",
    "X_tennis = tennis.drop(['play'],axis=1)\n",
    "y_tennis = pd.DataFrame(tennis['play'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train_tennis, X_valid_tennis, y_train_tennis, y_valid_tennis = train_test_split(X_tennis, y_tennis, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "y_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_missing_tennis = [col for col in tennis.columns if tennis[col].isnull().any()]\n",
    "cols_with_missing_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train datasets\n",
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the categorical variable to numeric variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlook  temp  humidity  windy\n",
       "2         0     1         0  False\n",
       "10        2     2         1   True\n",
       "4         1     0         1  False\n",
       "1         2     1         0   True\n",
       "12        0     1         1  False\n",
       "0         2     1         0  False\n",
       "13        1     2         0   True\n",
       "9         1     2         1  False\n",
       "8         2     0         1  False\n",
       "11        0     2         0   True\n",
       "5         1     0         1   True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cond = (X_train_tennis.dtypes == 'object')\n",
    "object_cols = list(cond[cond].index)\n",
    "\n",
    "label_X_train = X_train_tennis.copy()\n",
    "label_X_valid = X_valid_tennis.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train_tennis[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid_tennis[col])\n",
    "\n",
    "# In case needed\n",
    "label_y_train = label_encoder.fit_transform(y_train_tennis.values.ravel())\n",
    "label_y_valid = label_encoder.transform(y_valid_tennis.values.ravel())\n",
    "\n",
    "label_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis_classifier = DecisionTreeClassifier(random_state=1)\n",
    "tennis_classifier.fit(label_X_train, y_train_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- outlook <= 0.50\n",
      "|   |--- class: yes\n",
      "|--- outlook >  0.50\n",
      "|   |--- humidity <= 0.50\n",
      "|   |   |--- class: no\n",
      "|   |--- humidity >  0.50\n",
      "|   |   |--- windy <= 0.50\n",
      "|   |   |   |--- class: yes\n",
      "|   |   |--- windy >  0.50\n",
      "|   |   |   |--- outlook <= 1.50\n",
      "|   |   |   |   |--- class: no\n",
      "|   |   |   |--- outlook >  1.50\n",
      "|   |   |   |   |--- class: yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_tennis = export_text(tennis_classifier, feature_names = list(X_tennis.columns))\n",
    "print(tree_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict training data with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes',\n",
       "       'no'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_train_tennis = tennis_classifier.predict(label_X_train)\n",
    "predict_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to training data with Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_tennis = accuracy_score(y_train_tennis,predict_train_tennis)\n",
    "acc_train_tennis\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc_train_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to training data with F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_train_tennis = f1_score(y_train_tennis,predict_train_tennis,pos_label='yes')\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1_train_tennis)\n",
    "f1_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the test data using model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no', 'no', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_valid_tennis = tennis_classifier.predict(label_X_valid)\n",
    "predict_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with datatest using Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid_tennis = accuracy_score(y_valid_tennis,predict_valid_tennis)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc_valid_tennis)\n",
    "acc_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with datatest using F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid_tennis = f1_score(y_valid_tennis,predict_valid_tennis,pos_label='yes')\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1_valid_tennis)\n",
    "f1_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id3Estimator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train the model and print the tree produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outlook <=0.50: 1 (3) \n",
      "outlook >0.50\n",
      "|   humidity <=0.50: 0 (3) \n",
      "|   humidity >0.50\n",
      "|   |   windy <=0.50: 1 (3) \n",
      "|   |   windy >0.50\n",
      "|   |   |   temp <=1.00: 0 (1) \n",
      "|   |   |   temp >1.00: 1 (1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "id3_model2 = Id3Estimator()\n",
    "id3_model2.fit(label_X_train, label_y_train)\n",
    "\n",
    "tree_text2 = export.export_text(id3_model2.tree_, feature_names=list(tennis.columns))\n",
    "print(tree_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to training data with Accuracy and F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_train2 = id3_model2.predict(label_X_train)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_train2,label_y_train))\n",
    "print(\"F1 score: \", f1_score(predict_train2,label_y_train))\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(accuracy_score(predict_train2,label_y_train))\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1_score(predict_train2,label_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to test data with using Accuracy and F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predict_test2 = id3_model2.predict(label_X_valid)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_test2,label_y_valid))\n",
    "print(\"F1 score: \", f1_score(predict_test2,label_y_valid))\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(accuracy_score(predict_test2,label_y_valid))\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1_score(predict_test2,label_y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define KMeans and then train the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine how many clusters\n",
    "n_clst = 2\n",
    "# Define the KMeans\n",
    "kmeans_model_2 = KMeans(n_clusters = n_clst, random_state=1)\n",
    "kmeans_model_2.fit(label_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and produce clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = kmeans_model_2.predict(label_X_train)\n",
    "predict_test = kmeans_model_2.predict(label_X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the training data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.36363636363636365\n",
      "First clusterization F1: 0.4615384615384615\n",
      "Second clusterization accuracy: 0.6363636363636364\n",
      "Second clusterization F1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Find out each cluster belong with which class\n",
    "\n",
    "# First clusterization using first prediction (k_means_train)\n",
    "# Calculate the accuracy and F1\n",
    "acc_train = accuracy_score(label_y_train, predict_train)\n",
    "f1_train = f1_score(label_y_train, predict_train)\n",
    "print(f\"First clusterization accuracy: {acc_train}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_train = pd.Series(predict_train)\n",
    "fix = switched_train.apply(switch)\n",
    "acc_train1= accuracy_score(label_y_train, fix)\n",
    "f1_train2= f1_score(label_y_train, fix)\n",
    "print(f\"Second clusterization accuracy: {acc_train1}\")\n",
    "print(f\"Second clusterization F1: {f1_train2}\")\n",
    "\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc_train1)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second clusterization is better, so we take the metrics as the final metrics. Evaluate the validation data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.0\n",
      "First clusterization F1: 0.4615384615384615\n",
      "Second clusterization accuracy: 1.0\n",
      "Second clusterization F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First clusterization\n",
    "acc_valid = accuracy_score(label_y_valid, predict_test)\n",
    "f1_valid = f1_score(label_y_valid, predict_test)\n",
    "print(f\"First clusterization accuracy: {acc_valid}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_valid = pd.Series(predict_test).apply(switch)\n",
    "acc_valid_switch = accuracy_score(label_y_valid, switched_valid)\n",
    "f1_valid_switch = f1_score(label_y_valid, switched_valid)\n",
    "print(f\"Second clusterization accuracy: {acc_valid_switch}\")\n",
    "print(f\"Second clusterization F1: {f1_valid_switch}\")\n",
    "\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc_valid_switch)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1_valid_switch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as when we evaluate with training data, the second clusterization is better and then will be our final metrics score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = LogisticRegression(random_state=1)\n",
    "\n",
    "model_1.fit(label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the train data and evaluate the prediction with Accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local data\n",
      "Accuracy:  0.8181818181818182\n",
      "F1 score:  0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_1.predict(label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"local data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_1.predict(label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"test data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling : Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling if needed\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_label_X_train = scaler.fit_transform(label_X_train)\n",
    "scaled_label_X_valid = scaler.fit_transform(label_X_valid)\n",
    "\n",
    "model_2 = LogisticRegression(random_state=1)\n",
    "model_2.fit(scaled_label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local data\n",
      "Accuracy:  0.9090909090909091\n",
      "F1 score:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_2.predict(scaled_label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"local data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_2.predict(scaled_label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"test data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling : MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local data\n",
      "Accuracy:  0.9090909090909091\n",
      "F1 score:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "model_3 = LogisticRegression(random_state=1)\n",
    "model_3.fit(scaled_label_X_train,label_y_train)\n",
    "\n",
    "predict_train = model_3.predict(scaled_label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"local data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_3.predict(scaled_label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"test data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=700, random_state=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_1 = MLPClassifier(max_iter=700, random_state=1)\n",
    "\n",
    "model_1.fit(label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_1.predict(label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_1.predict(label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=700, random_state=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "\n",
    "scaled_label_X_train = scaler.fit_transform(label_X_train)\n",
    "scaled_label_X_valid = scaler.fit_transform(label_X_valid)\n",
    "\n",
    "model_2 = MLPClassifier(max_iter=700, random_state=1)\n",
    "model_2.fit(scaled_label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_2.predict(scaled_label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_2.predict(scaled_label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=700, random_state=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_label_X_train = scaler.fit_transform(label_X_train)\n",
    "scaled_label_X_valid = scaler.fit_transform(label_X_valid)\n",
    "\n",
    "model_3 = MLPClassifier(max_iter=700, random_state=1)\n",
    "model_3.fit(scaled_label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_train = model_3.predict(scaled_label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model_3.predict(scaled_label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the SVM (Support Vector Machines) constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(label_X_train, y_train_tennis.values.ravel())\n",
    "# .values will give the values in an array (shape: (n,1)\n",
    "# .ravel will convert that array shape to (n, )\n",
    "# y_train_tennis.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the data train and the data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes',\n",
       "       'yes'], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_svm_train_tennis = svm.predict(label_X_train)\n",
    "pred_svm_val_tennis = svm.predict(label_X_valid)\n",
    "pred_svm_train_tennis\n",
    "# pred_svm_val_tennis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the prediction of data train with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8181818181818182\n",
      "F1 :  0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "acc_score_svm = accuracy_score(y_train_tennis, pred_svm_train_tennis)\n",
    "# must define the positive label = yes\n",
    "f1_score_svm = f1_score(y_train_tennis, pred_svm_train_tennis, pos_label=\"yes\")\n",
    "print(\"Accuracy : \",acc_score_svm)\n",
    "print(\"F1 : \",f1_score_svm)\n",
    "# insert to dict\n",
    "data_tennis['acc_train'].append(acc_score_svm)\n",
    "# insert to dict\n",
    "data_tennis['f1_train'].append(f1_score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the prediction of data validation with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  1.0\n",
      "F1 :  1.0\n"
     ]
    }
   ],
   "source": [
    "acc_score_svm = accuracy_score(y_valid_tennis, pred_svm_val_tennis)\n",
    "# must define the positive label = yes\n",
    "f1_score_svm = f1_score(y_valid_tennis, pred_svm_val_tennis, pos_label=\"yes\")\n",
    "print(\"Accuracy : \",acc_score_svm)\n",
    "print(\"F1 : \",f1_score_svm)\n",
    "# insert to dict\n",
    "data_tennis['acc_valid'].append(acc_score_svm)\n",
    "# insert to dict\n",
    "data_tennis['f1_valid'].append(f1_score_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_tennis = pd.DataFrame(data_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis accuracy and F1 score for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The metrics data from Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_valid</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id3Estimator</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.938596</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>0.848352</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.891680</td>\n",
       "      <td>0.867470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logreg</td>\n",
       "      <td>0.960440</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.968531</td>\n",
       "      <td>0.959459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logreg_StdScaler</td>\n",
       "      <td>0.991209</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.993007</td>\n",
       "      <td>0.979310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logreg_MinMaxScaler</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.970940</td>\n",
       "      <td>0.979592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.955932</td>\n",
       "      <td>0.959459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NN_StdScaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964912</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN_MinMaxScaler</td>\n",
       "      <td>0.993407</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.994764</td>\n",
       "      <td>0.917293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.903509</td>\n",
       "      <td>0.940577</td>\n",
       "      <td>0.929032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     algo  acc_train  acc_valid  f1_train  f1_valid\n",
       "0  DecisionTreeClassifier   1.000000   0.947368  1.000000  0.959459\n",
       "1            Id3Estimator   1.000000   0.938596  1.000000  0.953020\n",
       "2                  KMeans   0.848352   0.807018  0.891680  0.867470\n",
       "3                  Logreg   0.960440   0.947368  0.968531  0.959459\n",
       "4        Logreg_StdScaler   0.991209   0.973684  0.993007  0.979310\n",
       "5     Logreg_MinMaxScaler   0.962637   0.973684  0.970940  0.979592\n",
       "6                      NN   0.942857   0.947368  0.955932  0.959459\n",
       "7            NN_StdScaler   1.000000   0.964912  1.000000  0.972222\n",
       "8         NN_MinMaxScaler   0.993407   0.903509  0.994764  0.917293\n",
       "9                     SVM   0.923077   0.903509  0.940577  0.929032"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis Tiap Model\n",
    "Decision Tree dan id3 Estimator:\n",
    "- akurasi dan f1 pada train set sempurna, namun cenderung turun pada validation set hal ini diakibatkan oleh kecenderungan dari terjadinya overfitting saat membentuk model Decision Tree, karena decision tree membentuk model dengan menyesuaikan rule-rule ke data yang sedang ditinjau. \n",
    "- pada data breast cancer, akurasi dan f1 validation dengan DTL biasa lebih baik daripada menggunakan id3 estimator\n",
    "\n",
    "KMeans:\n",
    "- akurasi dan f1 cenderung tidak terlalu tinggi, karena persebaran data tidak terbagi dua dengan jelas, dan kemungkinan terdapat outlier. Ada kemungkinan model mengambil centroid yang kurang tepat. Selain itu, KMeans merupakan algoritma unsupervised, sedangkan data yang kami gunakan memiliki label yang sebaiknya dapat dimanfaatkan untuk mempelajari data dengan lebih tepat\n",
    "\n",
    "Logistic Regression\n",
    "- akurasi dan f1 cenderung tinggi, agak mengalami overfit saat menggunakan standard scaler. Hal ini mungkin terjadi karena standard scaler mengubah nilai-nilai kolom pada range yang sama, sehingga hasil klasifikasi model pada data training cukup besar.\n",
    "\n",
    "Neural network\n",
    "- akurasi dan f1 tidak terlalu tinggi saat tidak menggunakan scaler\n",
    "- saat mengguakan scaler, akurasi bertambah namun mengalami overfit saat menggunakan minmax scaler\n",
    "\n",
    "SVM\n",
    "- akurasi dan f1 cenderung tidak terlalu tinggi. Kemungkinan hasil pengelompokan tidak memiliki dua bagian yang jelas sehingga sulit untuk membagi kedalam dua bagian data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kesimpulan\n",
    "Kasus 1:<br>\n",
    "Jika tidak memperhatikan model-model yang menggunakan <i>scaled dataset</i>, maka model-model yang akan dibandingkan adalah model 0, 1, 2, 3, 6, dan 9. Pertama-tama, kita melihat menggunakan <i>accuracy metrics</i> pada bagian <i>validation dataset</i>. Ketika dilihat, yang memiliki nilai <i>accuracy metrics</i> terbesar adalah 3 algoritma, yaitu DecisionTreeClassifier, LogisticRegression, dan NeuralNetwork dengan nilai sebesar 0.947368. Lalu, untuk melihat apakah algoritma <i>overfit</i> atau tidak, maka bisa dilihat pada bagian <i>training dataset</i>. Yang memiliki akurasi terkecil adalah NN. Lalu, kita cek pada bagian <i>F1 score metrics</i>, berlaku hal yang sama, NN memiliki F1 score terbesar pada validation dataset dan memiliki F1 score yang cukup kecil pada training datasetnya. Lalu, dalam hal ini KMeans memiliki performansi yang paling rendah, jika dilihat dari metrics dari tiap metode metrics.\n",
    "\n",
    "Kasus 2:<br>\n",
    "Jika memperhitungkan model-model yang menggunakan scaled dataset, maka Logistic Regression Model dengan Standard Scaler memiliki performansi terbaik, jika kita mempertimbangkan nilai accuracy metrics. Jika kita mempertimbangkan nilai F1 score, Logistic Regression Model dengan MinMaxScaler memiliki performansi terbaik. Yang jelas, jika Logistic Regression Model diberi Scaler, akan menjadi model yang paling baik. Untuk model yang memiliki performansi terburuk tetap jatuh pada KMeans, baik dari segi accuracy metrics maupun F1 score metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The metrics data from Play Tennis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algo</th>\n",
       "      <th>acc_train</th>\n",
       "      <th>acc_valid</th>\n",
       "      <th>f1_train</th>\n",
       "      <th>f1_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id3Estimator</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KMeans</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logreg</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logreg_StdScaler</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logreg_MinMaxScaler</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NN_StdScaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NN_MinMaxScaler</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     algo  acc_train  acc_valid  f1_train  f1_valid\n",
       "0  DecisionTreeClassifier   1.000000   0.666667  1.000000  0.666667\n",
       "1            Id3Estimator   1.000000   0.666667  1.000000  0.666667\n",
       "2                  KMeans   0.636364   1.000000  0.666667  1.000000\n",
       "3                  Logreg   0.818182   0.666667  0.875000  0.800000\n",
       "4        Logreg_StdScaler   0.909091   0.666667  0.933333  0.800000\n",
       "5     Logreg_MinMaxScaler   0.909091   0.666667  0.933333  0.800000\n",
       "6                      NN   1.000000   1.000000  1.000000  1.000000\n",
       "7            NN_StdScaler   1.000000   0.666667  1.000000  0.666667\n",
       "8         NN_MinMaxScaler   1.000000   0.666667  1.000000  0.800000\n",
       "9                     SVM   0.818182   1.000000  0.875000  1.000000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lalu, kita lihat pada dataset yang ukurannya lebih kecil -- bahkan jauh lebih kecil -- dari dataset sebelumnya. Jika kita menggunakan accuracy metrics sebagai acuan, maka ada tiga kandidat terbaik (jika melihat juga dari validation dataset), yaitu KMeans, NN, dan SVM. Kita lihat bagaimana kinerja ketika diterapkan pada training dataset. Hal yang cukup mengejutkan adalah hasil evaluasi pada validation dataset yang terjadi pada model KMeans dan SVM adalah lebih besar dari hasil evaluasi pada training dataset. Hal ini tidak biasa. Namun, yang jelas pada KMeans, selisihnya cukup jauh, sehingga tidak cukup baik untuk dijadikan pertimbangan, karena bisa saja data yang terjadi pada training dataset sangat random, sehingga banyak outlier daripada validation datasetnya. Lalu, ketika dilihat, NN sangat konsisten baik dalam accuracy metrics maupun F1 score metrics. Maka bisa disimpulkan NN memiliki performansi terbaik untuk dataset yang ukurannya kecil."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
