{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Made by:\n",
    "- Annisa Rahim / 13518089\n",
    "- Stefanus Gusega Gunawan / 13518149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast-Cancer Dataset\n",
    "\n",
    "Load the datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>17.99</td>\n",
       "      <td>20.66</td>\n",
       "      <td>117.80</td>\n",
       "      <td>991.7</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.13040</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.088240</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.06069</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080</td>\n",
       "      <td>25.41</td>\n",
       "      <td>138.10</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.37350</td>\n",
       "      <td>0.330100</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>0.08503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>9.00</td>\n",
       "      <td>14.40</td>\n",
       "      <td>56.36</td>\n",
       "      <td>246.3</td>\n",
       "      <td>0.07005</td>\n",
       "      <td>0.03116</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.003472</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>0.06833</td>\n",
       "      <td>...</td>\n",
       "      <td>9.699</td>\n",
       "      <td>20.07</td>\n",
       "      <td>60.90</td>\n",
       "      <td>285.5</td>\n",
       "      <td>0.09861</td>\n",
       "      <td>0.05232</td>\n",
       "      <td>0.014720</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.07804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>12.21</td>\n",
       "      <td>14.09</td>\n",
       "      <td>78.78</td>\n",
       "      <td>462.0</td>\n",
       "      <td>0.08108</td>\n",
       "      <td>0.07823</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>0.025340</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>0.06154</td>\n",
       "      <td>...</td>\n",
       "      <td>13.130</td>\n",
       "      <td>19.29</td>\n",
       "      <td>87.65</td>\n",
       "      <td>529.9</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.24310</td>\n",
       "      <td>0.307600</td>\n",
       "      <td>0.091400</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.08824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>12.34</td>\n",
       "      <td>14.95</td>\n",
       "      <td>78.29</td>\n",
       "      <td>469.1</td>\n",
       "      <td>0.08682</td>\n",
       "      <td>0.04571</td>\n",
       "      <td>0.021090</td>\n",
       "      <td>0.020540</td>\n",
       "      <td>0.1571</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.180</td>\n",
       "      <td>16.85</td>\n",
       "      <td>84.11</td>\n",
       "      <td>533.1</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.06744</td>\n",
       "      <td>0.049210</td>\n",
       "      <td>0.047930</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>0.05974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>14.86</td>\n",
       "      <td>16.94</td>\n",
       "      <td>94.89</td>\n",
       "      <td>673.7</td>\n",
       "      <td>0.08924</td>\n",
       "      <td>0.07074</td>\n",
       "      <td>0.033460</td>\n",
       "      <td>0.028770</td>\n",
       "      <td>0.1573</td>\n",
       "      <td>0.05703</td>\n",
       "      <td>...</td>\n",
       "      <td>16.310</td>\n",
       "      <td>20.54</td>\n",
       "      <td>102.30</td>\n",
       "      <td>777.5</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.15500</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.079710</td>\n",
       "      <td>0.2525</td>\n",
       "      <td>0.06827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.54</td>\n",
       "      <td>14.36</td>\n",
       "      <td>87.46</td>\n",
       "      <td>566.3</td>\n",
       "      <td>0.09779</td>\n",
       "      <td>0.08129</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.047810</td>\n",
       "      <td>0.1885</td>\n",
       "      <td>0.05766</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>19.26</td>\n",
       "      <td>99.70</td>\n",
       "      <td>711.2</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.17730</td>\n",
       "      <td>0.239000</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.2977</td>\n",
       "      <td>0.07259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>19.89</td>\n",
       "      <td>20.26</td>\n",
       "      <td>130.50</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.10370</td>\n",
       "      <td>0.13100</td>\n",
       "      <td>0.141100</td>\n",
       "      <td>0.094310</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.06188</td>\n",
       "      <td>...</td>\n",
       "      <td>23.730</td>\n",
       "      <td>25.23</td>\n",
       "      <td>160.50</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.33090</td>\n",
       "      <td>0.418500</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.09136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>20.55</td>\n",
       "      <td>20.86</td>\n",
       "      <td>137.80</td>\n",
       "      <td>1308.0</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.17390</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.2127</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>...</td>\n",
       "      <td>24.300</td>\n",
       "      <td>25.48</td>\n",
       "      <td>160.20</td>\n",
       "      <td>1809.0</td>\n",
       "      <td>0.12680</td>\n",
       "      <td>0.31350</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.07569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>11.99</td>\n",
       "      <td>24.89</td>\n",
       "      <td>77.61</td>\n",
       "      <td>441.3</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>0.09218</td>\n",
       "      <td>0.054410</td>\n",
       "      <td>0.042740</td>\n",
       "      <td>0.1820</td>\n",
       "      <td>0.06850</td>\n",
       "      <td>...</td>\n",
       "      <td>12.980</td>\n",
       "      <td>30.36</td>\n",
       "      <td>84.48</td>\n",
       "      <td>513.9</td>\n",
       "      <td>0.13110</td>\n",
       "      <td>0.18220</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.2599</td>\n",
       "      <td>0.08251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>12.88</td>\n",
       "      <td>28.92</td>\n",
       "      <td>82.50</td>\n",
       "      <td>514.3</td>\n",
       "      <td>0.08123</td>\n",
       "      <td>0.05824</td>\n",
       "      <td>0.061950</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.05708</td>\n",
       "      <td>...</td>\n",
       "      <td>13.890</td>\n",
       "      <td>35.74</td>\n",
       "      <td>88.84</td>\n",
       "      <td>595.7</td>\n",
       "      <td>0.12270</td>\n",
       "      <td>0.16200</td>\n",
       "      <td>0.243900</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.07242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>23.21</td>\n",
       "      <td>26.97</td>\n",
       "      <td>153.50</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>0.09509</td>\n",
       "      <td>0.16820</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.06309</td>\n",
       "      <td>...</td>\n",
       "      <td>31.010</td>\n",
       "      <td>34.51</td>\n",
       "      <td>206.00</td>\n",
       "      <td>2944.0</td>\n",
       "      <td>0.14810</td>\n",
       "      <td>0.41260</td>\n",
       "      <td>0.582000</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>0.08677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>14.87</td>\n",
       "      <td>16.67</td>\n",
       "      <td>98.64</td>\n",
       "      <td>682.5</td>\n",
       "      <td>0.11620</td>\n",
       "      <td>0.16490</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>0.089230</td>\n",
       "      <td>0.2157</td>\n",
       "      <td>0.06768</td>\n",
       "      <td>...</td>\n",
       "      <td>18.810</td>\n",
       "      <td>27.37</td>\n",
       "      <td>127.10</td>\n",
       "      <td>1095.0</td>\n",
       "      <td>0.18780</td>\n",
       "      <td>0.44800</td>\n",
       "      <td>0.470400</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.3585</td>\n",
       "      <td>0.10650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>16.84</td>\n",
       "      <td>19.46</td>\n",
       "      <td>108.40</td>\n",
       "      <td>880.2</td>\n",
       "      <td>0.07445</td>\n",
       "      <td>0.07223</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.027710</td>\n",
       "      <td>0.1844</td>\n",
       "      <td>0.05268</td>\n",
       "      <td>...</td>\n",
       "      <td>18.220</td>\n",
       "      <td>28.07</td>\n",
       "      <td>120.30</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>0.08774</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.188200</td>\n",
       "      <td>0.084360</td>\n",
       "      <td>0.2527</td>\n",
       "      <td>0.05972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>19.59</td>\n",
       "      <td>18.15</td>\n",
       "      <td>130.70</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.16660</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>0.06082</td>\n",
       "      <td>...</td>\n",
       "      <td>26.730</td>\n",
       "      <td>26.39</td>\n",
       "      <td>174.90</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>0.14380</td>\n",
       "      <td>0.38460</td>\n",
       "      <td>0.681000</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.3643</td>\n",
       "      <td>0.09223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>20.18</td>\n",
       "      <td>23.97</td>\n",
       "      <td>143.70</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>0.12860</td>\n",
       "      <td>0.34540</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.160400</td>\n",
       "      <td>0.2906</td>\n",
       "      <td>0.08142</td>\n",
       "      <td>...</td>\n",
       "      <td>23.370</td>\n",
       "      <td>31.72</td>\n",
       "      <td>170.30</td>\n",
       "      <td>1623.0</td>\n",
       "      <td>0.16390</td>\n",
       "      <td>0.61640</td>\n",
       "      <td>0.768100</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>0.09964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>12.27</td>\n",
       "      <td>17.92</td>\n",
       "      <td>78.41</td>\n",
       "      <td>466.1</td>\n",
       "      <td>0.08685</td>\n",
       "      <td>0.06526</td>\n",
       "      <td>0.032110</td>\n",
       "      <td>0.026530</td>\n",
       "      <td>0.1966</td>\n",
       "      <td>0.05597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.100</td>\n",
       "      <td>28.88</td>\n",
       "      <td>89.00</td>\n",
       "      <td>610.2</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0.17950</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.095320</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>0.06896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>15.73</td>\n",
       "      <td>11.28</td>\n",
       "      <td>102.80</td>\n",
       "      <td>747.2</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.12990</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.062110</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>0.06259</td>\n",
       "      <td>...</td>\n",
       "      <td>17.010</td>\n",
       "      <td>14.20</td>\n",
       "      <td>112.50</td>\n",
       "      <td>854.3</td>\n",
       "      <td>0.15410</td>\n",
       "      <td>0.29790</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.145200</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.08181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>12.30</td>\n",
       "      <td>19.02</td>\n",
       "      <td>77.88</td>\n",
       "      <td>464.4</td>\n",
       "      <td>0.08313</td>\n",
       "      <td>0.04202</td>\n",
       "      <td>0.007756</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.05945</td>\n",
       "      <td>...</td>\n",
       "      <td>13.350</td>\n",
       "      <td>28.46</td>\n",
       "      <td>84.53</td>\n",
       "      <td>544.3</td>\n",
       "      <td>0.12220</td>\n",
       "      <td>0.09052</td>\n",
       "      <td>0.036190</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>0.2554</td>\n",
       "      <td>0.07207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>19.07</td>\n",
       "      <td>24.81</td>\n",
       "      <td>128.30</td>\n",
       "      <td>1104.0</td>\n",
       "      <td>0.09081</td>\n",
       "      <td>0.21900</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.099610</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.06343</td>\n",
       "      <td>...</td>\n",
       "      <td>24.090</td>\n",
       "      <td>33.17</td>\n",
       "      <td>177.40</td>\n",
       "      <td>1651.0</td>\n",
       "      <td>0.12470</td>\n",
       "      <td>0.74440</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.4670</td>\n",
       "      <td>0.10380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>11.08</td>\n",
       "      <td>14.71</td>\n",
       "      <td>70.21</td>\n",
       "      <td>372.7</td>\n",
       "      <td>0.10060</td>\n",
       "      <td>0.05743</td>\n",
       "      <td>0.023630</td>\n",
       "      <td>0.025830</td>\n",
       "      <td>0.1566</td>\n",
       "      <td>0.06669</td>\n",
       "      <td>...</td>\n",
       "      <td>11.350</td>\n",
       "      <td>16.82</td>\n",
       "      <td>72.01</td>\n",
       "      <td>396.5</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.08240</td>\n",
       "      <td>0.039380</td>\n",
       "      <td>0.043060</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.07313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>15.75</td>\n",
       "      <td>20.25</td>\n",
       "      <td>102.60</td>\n",
       "      <td>761.3</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.12040</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.064620</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.06303</td>\n",
       "      <td>...</td>\n",
       "      <td>19.560</td>\n",
       "      <td>30.29</td>\n",
       "      <td>125.90</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>0.15520</td>\n",
       "      <td>0.44800</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.147900</td>\n",
       "      <td>0.3993</td>\n",
       "      <td>0.10640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>17.54</td>\n",
       "      <td>19.32</td>\n",
       "      <td>115.10</td>\n",
       "      <td>951.6</td>\n",
       "      <td>0.08968</td>\n",
       "      <td>0.11980</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.074880</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.05491</td>\n",
       "      <td>...</td>\n",
       "      <td>20.420</td>\n",
       "      <td>25.84</td>\n",
       "      <td>139.50</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>0.13810</td>\n",
       "      <td>0.34200</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.193900</td>\n",
       "      <td>0.2928</td>\n",
       "      <td>0.07867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>15.71</td>\n",
       "      <td>13.93</td>\n",
       "      <td>102.00</td>\n",
       "      <td>761.7</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>0.09462</td>\n",
       "      <td>0.071350</td>\n",
       "      <td>0.059330</td>\n",
       "      <td>0.1816</td>\n",
       "      <td>0.05723</td>\n",
       "      <td>...</td>\n",
       "      <td>17.500</td>\n",
       "      <td>19.25</td>\n",
       "      <td>114.30</td>\n",
       "      <td>922.8</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.19490</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.137400</td>\n",
       "      <td>0.2723</td>\n",
       "      <td>0.07071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>11.22</td>\n",
       "      <td>33.81</td>\n",
       "      <td>70.79</td>\n",
       "      <td>386.8</td>\n",
       "      <td>0.07780</td>\n",
       "      <td>0.03574</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>0.05828</td>\n",
       "      <td>...</td>\n",
       "      <td>12.360</td>\n",
       "      <td>41.78</td>\n",
       "      <td>78.44</td>\n",
       "      <td>470.9</td>\n",
       "      <td>0.09994</td>\n",
       "      <td>0.06885</td>\n",
       "      <td>0.023180</td>\n",
       "      <td>0.030020</td>\n",
       "      <td>0.2911</td>\n",
       "      <td>0.07307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>14.99</td>\n",
       "      <td>22.11</td>\n",
       "      <td>97.53</td>\n",
       "      <td>693.7</td>\n",
       "      <td>0.08515</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.068590</td>\n",
       "      <td>0.038760</td>\n",
       "      <td>0.1944</td>\n",
       "      <td>0.05913</td>\n",
       "      <td>...</td>\n",
       "      <td>16.760</td>\n",
       "      <td>31.55</td>\n",
       "      <td>110.20</td>\n",
       "      <td>867.1</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.33450</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.130800</td>\n",
       "      <td>0.3163</td>\n",
       "      <td>0.09251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>15.05</td>\n",
       "      <td>19.07</td>\n",
       "      <td>97.26</td>\n",
       "      <td>701.9</td>\n",
       "      <td>0.09215</td>\n",
       "      <td>0.08597</td>\n",
       "      <td>0.074860</td>\n",
       "      <td>0.043350</td>\n",
       "      <td>0.1561</td>\n",
       "      <td>0.05915</td>\n",
       "      <td>...</td>\n",
       "      <td>17.580</td>\n",
       "      <td>28.06</td>\n",
       "      <td>113.80</td>\n",
       "      <td>967.0</td>\n",
       "      <td>0.12460</td>\n",
       "      <td>0.21010</td>\n",
       "      <td>0.286600</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>0.06954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>19.40</td>\n",
       "      <td>23.50</td>\n",
       "      <td>129.10</td>\n",
       "      <td>1155.0</td>\n",
       "      <td>0.10270</td>\n",
       "      <td>0.15580</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.088860</td>\n",
       "      <td>0.1978</td>\n",
       "      <td>0.06000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.650</td>\n",
       "      <td>30.53</td>\n",
       "      <td>144.90</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>0.14630</td>\n",
       "      <td>0.29680</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.2920</td>\n",
       "      <td>0.07614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>11.89</td>\n",
       "      <td>21.17</td>\n",
       "      <td>76.39</td>\n",
       "      <td>433.8</td>\n",
       "      <td>0.09773</td>\n",
       "      <td>0.08120</td>\n",
       "      <td>0.025550</td>\n",
       "      <td>0.021790</td>\n",
       "      <td>0.2019</td>\n",
       "      <td>0.06290</td>\n",
       "      <td>...</td>\n",
       "      <td>13.050</td>\n",
       "      <td>27.21</td>\n",
       "      <td>85.09</td>\n",
       "      <td>522.9</td>\n",
       "      <td>0.14260</td>\n",
       "      <td>0.21870</td>\n",
       "      <td>0.116400</td>\n",
       "      <td>0.082630</td>\n",
       "      <td>0.3075</td>\n",
       "      <td>0.07351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>15.22</td>\n",
       "      <td>30.62</td>\n",
       "      <td>103.40</td>\n",
       "      <td>716.9</td>\n",
       "      <td>0.10480</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.094290</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.07152</td>\n",
       "      <td>...</td>\n",
       "      <td>17.520</td>\n",
       "      <td>42.79</td>\n",
       "      <td>128.70</td>\n",
       "      <td>915.0</td>\n",
       "      <td>0.14170</td>\n",
       "      <td>0.79170</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>0.14090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>17.19</td>\n",
       "      <td>22.07</td>\n",
       "      <td>111.60</td>\n",
       "      <td>928.3</td>\n",
       "      <td>0.09726</td>\n",
       "      <td>0.08995</td>\n",
       "      <td>0.090610</td>\n",
       "      <td>0.065270</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>21.580</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.50</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.15580</td>\n",
       "      <td>0.25670</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.07570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>15.27</td>\n",
       "      <td>12.91</td>\n",
       "      <td>98.17</td>\n",
       "      <td>725.5</td>\n",
       "      <td>0.08182</td>\n",
       "      <td>0.06230</td>\n",
       "      <td>0.058920</td>\n",
       "      <td>0.031570</td>\n",
       "      <td>0.1359</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>...</td>\n",
       "      <td>17.380</td>\n",
       "      <td>15.92</td>\n",
       "      <td>113.70</td>\n",
       "      <td>932.7</td>\n",
       "      <td>0.12220</td>\n",
       "      <td>0.21860</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.103500</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.07474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>12.18</td>\n",
       "      <td>14.08</td>\n",
       "      <td>77.25</td>\n",
       "      <td>461.4</td>\n",
       "      <td>0.07734</td>\n",
       "      <td>0.03212</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>0.05649</td>\n",
       "      <td>...</td>\n",
       "      <td>12.850</td>\n",
       "      <td>16.47</td>\n",
       "      <td>81.60</td>\n",
       "      <td>513.1</td>\n",
       "      <td>0.10010</td>\n",
       "      <td>0.05332</td>\n",
       "      <td>0.041160</td>\n",
       "      <td>0.018520</td>\n",
       "      <td>0.2293</td>\n",
       "      <td>0.06037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>14.58</td>\n",
       "      <td>13.66</td>\n",
       "      <td>94.29</td>\n",
       "      <td>658.8</td>\n",
       "      <td>0.09832</td>\n",
       "      <td>0.08918</td>\n",
       "      <td>0.082220</td>\n",
       "      <td>0.043490</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05640</td>\n",
       "      <td>...</td>\n",
       "      <td>16.760</td>\n",
       "      <td>17.24</td>\n",
       "      <td>108.50</td>\n",
       "      <td>862.0</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.19280</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.091860</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>0.07048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>11.54</td>\n",
       "      <td>10.72</td>\n",
       "      <td>73.73</td>\n",
       "      <td>409.1</td>\n",
       "      <td>0.08597</td>\n",
       "      <td>0.05969</td>\n",
       "      <td>0.013670</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.06100</td>\n",
       "      <td>...</td>\n",
       "      <td>12.340</td>\n",
       "      <td>12.87</td>\n",
       "      <td>81.23</td>\n",
       "      <td>467.8</td>\n",
       "      <td>0.10920</td>\n",
       "      <td>0.16260</td>\n",
       "      <td>0.083240</td>\n",
       "      <td>0.047150</td>\n",
       "      <td>0.3390</td>\n",
       "      <td>0.07434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>10.96</td>\n",
       "      <td>17.62</td>\n",
       "      <td>70.79</td>\n",
       "      <td>365.6</td>\n",
       "      <td>0.09687</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.027880</td>\n",
       "      <td>0.1619</td>\n",
       "      <td>0.06408</td>\n",
       "      <td>...</td>\n",
       "      <td>11.620</td>\n",
       "      <td>26.51</td>\n",
       "      <td>76.43</td>\n",
       "      <td>407.5</td>\n",
       "      <td>0.14280</td>\n",
       "      <td>0.25100</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.098610</td>\n",
       "      <td>0.2289</td>\n",
       "      <td>0.08278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>12.43</td>\n",
       "      <td>17.00</td>\n",
       "      <td>78.60</td>\n",
       "      <td>477.3</td>\n",
       "      <td>0.07557</td>\n",
       "      <td>0.03454</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.016990</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.05561</td>\n",
       "      <td>...</td>\n",
       "      <td>12.900</td>\n",
       "      <td>20.21</td>\n",
       "      <td>81.76</td>\n",
       "      <td>515.9</td>\n",
       "      <td>0.08409</td>\n",
       "      <td>0.04712</td>\n",
       "      <td>0.022370</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.05932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.059850</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.060</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.16540</td>\n",
       "      <td>0.36820</td>\n",
       "      <td>0.267800</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>21.61</td>\n",
       "      <td>22.28</td>\n",
       "      <td>144.40</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>0.11670</td>\n",
       "      <td>0.20870</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>0.156200</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.06606</td>\n",
       "      <td>...</td>\n",
       "      <td>26.230</td>\n",
       "      <td>28.74</td>\n",
       "      <td>172.00</td>\n",
       "      <td>2081.0</td>\n",
       "      <td>0.15020</td>\n",
       "      <td>0.57170</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.3828</td>\n",
       "      <td>0.10070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>16.11</td>\n",
       "      <td>18.05</td>\n",
       "      <td>105.10</td>\n",
       "      <td>813.0</td>\n",
       "      <td>0.09721</td>\n",
       "      <td>0.11370</td>\n",
       "      <td>0.094470</td>\n",
       "      <td>0.059430</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.06248</td>\n",
       "      <td>...</td>\n",
       "      <td>19.920</td>\n",
       "      <td>25.27</td>\n",
       "      <td>129.00</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>0.13140</td>\n",
       "      <td>0.22360</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.2792</td>\n",
       "      <td>0.08158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>14.48</td>\n",
       "      <td>21.46</td>\n",
       "      <td>94.25</td>\n",
       "      <td>648.2</td>\n",
       "      <td>0.09444</td>\n",
       "      <td>0.09947</td>\n",
       "      <td>0.120400</td>\n",
       "      <td>0.049380</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.05636</td>\n",
       "      <td>...</td>\n",
       "      <td>16.210</td>\n",
       "      <td>29.25</td>\n",
       "      <td>108.40</td>\n",
       "      <td>808.9</td>\n",
       "      <td>0.13060</td>\n",
       "      <td>0.19760</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>0.06846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>11.49</td>\n",
       "      <td>14.59</td>\n",
       "      <td>73.99</td>\n",
       "      <td>404.9</td>\n",
       "      <td>0.10460</td>\n",
       "      <td>0.08228</td>\n",
       "      <td>0.053080</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.06574</td>\n",
       "      <td>...</td>\n",
       "      <td>12.400</td>\n",
       "      <td>21.90</td>\n",
       "      <td>82.04</td>\n",
       "      <td>467.6</td>\n",
       "      <td>0.13520</td>\n",
       "      <td>0.20100</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>0.09180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>23.09</td>\n",
       "      <td>19.83</td>\n",
       "      <td>152.10</td>\n",
       "      <td>1682.0</td>\n",
       "      <td>0.09342</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.167600</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.1505</td>\n",
       "      <td>0.05484</td>\n",
       "      <td>...</td>\n",
       "      <td>30.790</td>\n",
       "      <td>23.87</td>\n",
       "      <td>211.50</td>\n",
       "      <td>2782.0</td>\n",
       "      <td>0.11990</td>\n",
       "      <td>0.36250</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.226400</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.07277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>13.86</td>\n",
       "      <td>16.93</td>\n",
       "      <td>90.96</td>\n",
       "      <td>578.9</td>\n",
       "      <td>0.10260</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.099010</td>\n",
       "      <td>0.056020</td>\n",
       "      <td>0.2106</td>\n",
       "      <td>0.06916</td>\n",
       "      <td>...</td>\n",
       "      <td>15.750</td>\n",
       "      <td>26.93</td>\n",
       "      <td>104.40</td>\n",
       "      <td>750.1</td>\n",
       "      <td>0.14600</td>\n",
       "      <td>0.43700</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.165400</td>\n",
       "      <td>0.3630</td>\n",
       "      <td>0.10590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>11.06</td>\n",
       "      <td>14.83</td>\n",
       "      <td>70.31</td>\n",
       "      <td>378.2</td>\n",
       "      <td>0.07741</td>\n",
       "      <td>0.04768</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.1535</td>\n",
       "      <td>0.06214</td>\n",
       "      <td>...</td>\n",
       "      <td>12.680</td>\n",
       "      <td>20.35</td>\n",
       "      <td>80.79</td>\n",
       "      <td>496.7</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.18790</td>\n",
       "      <td>0.207900</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.2590</td>\n",
       "      <td>0.09158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>12.25</td>\n",
       "      <td>22.44</td>\n",
       "      <td>78.18</td>\n",
       "      <td>466.5</td>\n",
       "      <td>0.08192</td>\n",
       "      <td>0.05200</td>\n",
       "      <td>0.017140</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>0.05976</td>\n",
       "      <td>...</td>\n",
       "      <td>14.170</td>\n",
       "      <td>31.99</td>\n",
       "      <td>92.74</td>\n",
       "      <td>622.9</td>\n",
       "      <td>0.12560</td>\n",
       "      <td>0.18040</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.063350</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.08203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>19.73</td>\n",
       "      <td>19.82</td>\n",
       "      <td>130.70</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.18490</td>\n",
       "      <td>0.241700</td>\n",
       "      <td>0.097400</td>\n",
       "      <td>0.1733</td>\n",
       "      <td>0.06697</td>\n",
       "      <td>...</td>\n",
       "      <td>25.280</td>\n",
       "      <td>25.59</td>\n",
       "      <td>159.80</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>0.17100</td>\n",
       "      <td>0.59550</td>\n",
       "      <td>0.848900</td>\n",
       "      <td>0.250700</td>\n",
       "      <td>0.2749</td>\n",
       "      <td>0.12970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>17.60</td>\n",
       "      <td>23.33</td>\n",
       "      <td>119.00</td>\n",
       "      <td>980.5</td>\n",
       "      <td>0.09289</td>\n",
       "      <td>0.20040</td>\n",
       "      <td>0.213600</td>\n",
       "      <td>0.100200</td>\n",
       "      <td>0.1696</td>\n",
       "      <td>0.07369</td>\n",
       "      <td>...</td>\n",
       "      <td>21.570</td>\n",
       "      <td>28.87</td>\n",
       "      <td>143.60</td>\n",
       "      <td>1437.0</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.47850</td>\n",
       "      <td>0.516500</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.12240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>13.87</td>\n",
       "      <td>16.21</td>\n",
       "      <td>88.52</td>\n",
       "      <td>593.7</td>\n",
       "      <td>0.08743</td>\n",
       "      <td>0.05492</td>\n",
       "      <td>0.015020</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>15.110</td>\n",
       "      <td>25.58</td>\n",
       "      <td>96.74</td>\n",
       "      <td>694.4</td>\n",
       "      <td>0.11530</td>\n",
       "      <td>0.10080</td>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.055560</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>0.07113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>19.45</td>\n",
       "      <td>19.33</td>\n",
       "      <td>126.50</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>0.10350</td>\n",
       "      <td>0.11880</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.085910</td>\n",
       "      <td>0.1776</td>\n",
       "      <td>0.05647</td>\n",
       "      <td>...</td>\n",
       "      <td>25.700</td>\n",
       "      <td>24.57</td>\n",
       "      <td>163.10</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>0.14970</td>\n",
       "      <td>0.31610</td>\n",
       "      <td>0.431700</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.3379</td>\n",
       "      <td>0.08950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>11.33</td>\n",
       "      <td>14.16</td>\n",
       "      <td>71.79</td>\n",
       "      <td>396.6</td>\n",
       "      <td>0.09379</td>\n",
       "      <td>0.03872</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.1954</td>\n",
       "      <td>0.05821</td>\n",
       "      <td>...</td>\n",
       "      <td>12.200</td>\n",
       "      <td>18.99</td>\n",
       "      <td>77.37</td>\n",
       "      <td>458.0</td>\n",
       "      <td>0.12590</td>\n",
       "      <td>0.07348</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.011110</td>\n",
       "      <td>0.2758</td>\n",
       "      <td>0.06386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>13.01</td>\n",
       "      <td>22.22</td>\n",
       "      <td>82.01</td>\n",
       "      <td>526.4</td>\n",
       "      <td>0.06251</td>\n",
       "      <td>0.01938</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.05234</td>\n",
       "      <td>...</td>\n",
       "      <td>14.000</td>\n",
       "      <td>29.02</td>\n",
       "      <td>88.18</td>\n",
       "      <td>608.8</td>\n",
       "      <td>0.08125</td>\n",
       "      <td>0.03432</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.05843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>11.74</td>\n",
       "      <td>14.02</td>\n",
       "      <td>74.24</td>\n",
       "      <td>427.3</td>\n",
       "      <td>0.07813</td>\n",
       "      <td>0.04340</td>\n",
       "      <td>0.022450</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.2101</td>\n",
       "      <td>0.06113</td>\n",
       "      <td>...</td>\n",
       "      <td>13.310</td>\n",
       "      <td>18.26</td>\n",
       "      <td>84.70</td>\n",
       "      <td>533.7</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.08500</td>\n",
       "      <td>0.067350</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>0.06688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>10.26</td>\n",
       "      <td>12.22</td>\n",
       "      <td>65.75</td>\n",
       "      <td>321.6</td>\n",
       "      <td>0.09996</td>\n",
       "      <td>0.07542</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>0.019680</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.06569</td>\n",
       "      <td>...</td>\n",
       "      <td>11.380</td>\n",
       "      <td>15.65</td>\n",
       "      <td>73.23</td>\n",
       "      <td>394.5</td>\n",
       "      <td>0.13430</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.086150</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.2937</td>\n",
       "      <td>0.07722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>16.30</td>\n",
       "      <td>15.70</td>\n",
       "      <td>104.70</td>\n",
       "      <td>819.8</td>\n",
       "      <td>0.09427</td>\n",
       "      <td>0.06712</td>\n",
       "      <td>0.055260</td>\n",
       "      <td>0.045630</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>0.05657</td>\n",
       "      <td>...</td>\n",
       "      <td>17.320</td>\n",
       "      <td>17.76</td>\n",
       "      <td>109.80</td>\n",
       "      <td>928.2</td>\n",
       "      <td>0.13540</td>\n",
       "      <td>0.13610</td>\n",
       "      <td>0.194700</td>\n",
       "      <td>0.135700</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.07230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>19.79</td>\n",
       "      <td>25.12</td>\n",
       "      <td>130.40</td>\n",
       "      <td>1192.0</td>\n",
       "      <td>0.10150</td>\n",
       "      <td>0.15890</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.2202</td>\n",
       "      <td>0.06113</td>\n",
       "      <td>...</td>\n",
       "      <td>22.630</td>\n",
       "      <td>33.58</td>\n",
       "      <td>148.70</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.38610</td>\n",
       "      <td>0.567300</td>\n",
       "      <td>0.173200</td>\n",
       "      <td>0.3305</td>\n",
       "      <td>0.08465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>10.75</td>\n",
       "      <td>14.97</td>\n",
       "      <td>68.26</td>\n",
       "      <td>355.3</td>\n",
       "      <td>0.07793</td>\n",
       "      <td>0.05139</td>\n",
       "      <td>0.022510</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.1399</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>...</td>\n",
       "      <td>11.950</td>\n",
       "      <td>20.72</td>\n",
       "      <td>77.79</td>\n",
       "      <td>441.2</td>\n",
       "      <td>0.10760</td>\n",
       "      <td>0.12230</td>\n",
       "      <td>0.097550</td>\n",
       "      <td>0.034130</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.06769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>17.20</td>\n",
       "      <td>24.52</td>\n",
       "      <td>114.20</td>\n",
       "      <td>929.4</td>\n",
       "      <td>0.10710</td>\n",
       "      <td>0.18300</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.1927</td>\n",
       "      <td>0.06487</td>\n",
       "      <td>...</td>\n",
       "      <td>23.320</td>\n",
       "      <td>33.82</td>\n",
       "      <td>151.60</td>\n",
       "      <td>1681.0</td>\n",
       "      <td>0.15850</td>\n",
       "      <td>0.73940</td>\n",
       "      <td>0.656600</td>\n",
       "      <td>0.189900</td>\n",
       "      <td>0.3313</td>\n",
       "      <td>0.13390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>14.03</td>\n",
       "      <td>21.25</td>\n",
       "      <td>89.79</td>\n",
       "      <td>603.4</td>\n",
       "      <td>0.09070</td>\n",
       "      <td>0.06945</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>0.018960</td>\n",
       "      <td>0.1517</td>\n",
       "      <td>0.05835</td>\n",
       "      <td>...</td>\n",
       "      <td>15.330</td>\n",
       "      <td>30.28</td>\n",
       "      <td>98.27</td>\n",
       "      <td>715.5</td>\n",
       "      <td>0.12870</td>\n",
       "      <td>0.15130</td>\n",
       "      <td>0.062310</td>\n",
       "      <td>0.079630</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.07617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13.03</td>\n",
       "      <td>18.42</td>\n",
       "      <td>82.61</td>\n",
       "      <td>523.8</td>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.03766</td>\n",
       "      <td>0.025620</td>\n",
       "      <td>0.029230</td>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.05863</td>\n",
       "      <td>...</td>\n",
       "      <td>13.300</td>\n",
       "      <td>22.81</td>\n",
       "      <td>84.46</td>\n",
       "      <td>545.9</td>\n",
       "      <td>0.09701</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.048330</td>\n",
       "      <td>0.050130</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.06169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>455 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "408        17.99         20.66          117.80      991.7          0.10360   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "307         9.00         14.40           56.36      246.3          0.07005   \n",
       "386        12.21         14.09           78.78      462.0          0.08108   \n",
       "404        12.34         14.95           78.29      469.1          0.08682   \n",
       "434        14.86         16.94           94.89      673.7          0.08924   \n",
       "19         13.54         14.36           87.46      566.3          0.09779   \n",
       "517        19.89         20.26          130.50     1214.0          0.10370   \n",
       "535        20.55         20.86          137.80     1308.0          0.10460   \n",
       "445        11.99         24.89           77.61      441.3          0.10300   \n",
       "554        12.88         28.92           82.50      514.3          0.08123   \n",
       "236        23.21         26.97          153.50     1670.0          0.09509   \n",
       "117        14.87         16.67           98.64      682.5          0.11620   \n",
       "157        16.84         19.46          108.40      880.2          0.07445   \n",
       "162        19.59         18.15          130.70     1214.0          0.11200   \n",
       "78         20.18         23.97          143.70     1245.0          0.12860   \n",
       "409        12.27         17.92           78.41      466.1          0.08685   \n",
       "484        15.73         11.28          102.80      747.2          0.10430   \n",
       "334        12.30         19.02           77.88      464.4          0.08313   \n",
       "42         19.07         24.81          128.30     1104.0          0.09081   \n",
       "173        11.08         14.71           70.21      372.7          0.10060   \n",
       "223        15.75         20.25          102.60      761.3          0.10250   \n",
       "201        17.54         19.32          115.10      951.6          0.08968   \n",
       "133        15.71         13.93          102.00      761.7          0.09462   \n",
       "232        11.22         33.81           70.79      386.8          0.07780   \n",
       "413        14.99         22.11           97.53      693.7          0.08515   \n",
       "514        15.05         19.07           97.26      701.9          0.09215   \n",
       "244        19.40         23.50          129.10     1155.0          0.10270   \n",
       "415        11.89         21.17           76.39      433.8          0.09773   \n",
       "562        15.22         30.62          103.40      716.9          0.10480   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "264        17.19         22.07          111.60      928.3          0.09726   \n",
       "209        15.27         12.91           98.17      725.5          0.08182   \n",
       "316        12.18         14.08           77.25      461.4          0.07734   \n",
       "513        14.58         13.66           94.29      658.8          0.09832   \n",
       "313        11.54         10.72           73.73      409.1          0.08597   \n",
       "534        10.96         17.62           70.79      365.6          0.09687   \n",
       "319        12.43         17.00           78.60      477.3          0.07557   \n",
       "7          13.71         20.83           90.20      577.9          0.11890   \n",
       "393        21.61         22.28          144.40     1407.0          0.11670   \n",
       "141        16.11         18.05          105.10      813.0          0.09721   \n",
       "86         14.48         21.46           94.25      648.2          0.09444   \n",
       "478        11.49         14.59           73.99      404.9          0.10460   \n",
       "503        23.09         19.83          152.10     1682.0          0.09342   \n",
       "215        13.86         16.93           90.96      578.9          0.10260   \n",
       "398        11.06         14.83           70.31      378.2          0.07741   \n",
       "490        12.25         22.44           78.18      466.5          0.08192   \n",
       "252        19.73         19.82          130.70     1206.0          0.10620   \n",
       "468        17.60         23.33          119.00      980.5          0.09289   \n",
       "357        13.87         16.21           88.52      593.7          0.08743   \n",
       "254        19.45         19.33          126.50     1169.0          0.10350   \n",
       "276        11.33         14.16           71.79      396.6          0.09379   \n",
       "178        13.01         22.22           82.01      526.4          0.06251   \n",
       "281        11.74         14.02           74.24      427.3          0.07813   \n",
       "390        10.26         12.22           65.75      321.6          0.09996   \n",
       "508        16.30         15.70          104.70      819.8          0.09427   \n",
       "129        19.79         25.12          130.40     1192.0          0.10150   \n",
       "144        10.75         14.97           68.26      355.3          0.07793   \n",
       "72         17.20         24.52          114.20      929.4          0.10710   \n",
       "235        14.03         21.25           89.79      603.4          0.09070   \n",
       "37         13.03         18.42           82.61      523.8          0.08983   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "408           0.13040        0.120100             0.088240         0.1992   \n",
       "4             0.13280        0.198000             0.104300         0.1809   \n",
       "307           0.03116        0.003681             0.003472         0.1788   \n",
       "386           0.07823        0.068390             0.025340         0.1646   \n",
       "404           0.04571        0.021090             0.020540         0.1571   \n",
       "434           0.07074        0.033460             0.028770         0.1573   \n",
       "19            0.08129        0.066640             0.047810         0.1885   \n",
       "517           0.13100        0.141100             0.094310         0.1802   \n",
       "535           0.17390        0.208500             0.132200         0.2127   \n",
       "445           0.09218        0.054410             0.042740         0.1820   \n",
       "554           0.05824        0.061950             0.023430         0.1566   \n",
       "236           0.16820        0.195000             0.123700         0.1909   \n",
       "117           0.16490        0.169000             0.089230         0.2157   \n",
       "157           0.07223        0.051500             0.027710         0.1844   \n",
       "162           0.16660        0.250800             0.128600         0.2027   \n",
       "78            0.34540        0.375400             0.160400         0.2906   \n",
       "409           0.06526        0.032110             0.026530         0.1966   \n",
       "484           0.12990        0.119100             0.062110         0.1784   \n",
       "334           0.04202        0.007756             0.008535         0.1539   \n",
       "42            0.21900        0.210700             0.099610         0.2310   \n",
       "173           0.05743        0.023630             0.025830         0.1566   \n",
       "223           0.12040        0.114700             0.064620         0.1935   \n",
       "201           0.11980        0.103600             0.074880         0.1506   \n",
       "133           0.09462        0.071350             0.059330         0.1816   \n",
       "232           0.03574        0.004967             0.006434         0.1845   \n",
       "413           0.10250        0.068590             0.038760         0.1944   \n",
       "514           0.08597        0.074860             0.043350         0.1561   \n",
       "244           0.15580        0.204900             0.088860         0.1978   \n",
       "415           0.08120        0.025550             0.021790         0.2019   \n",
       "562           0.20870        0.255000             0.094290         0.2128   \n",
       "..                ...             ...                  ...            ...   \n",
       "264           0.08995        0.090610             0.065270         0.1867   \n",
       "209           0.06230        0.058920             0.031570         0.1359   \n",
       "316           0.03212        0.011230             0.005051         0.1673   \n",
       "513           0.08918        0.082220             0.043490         0.1739   \n",
       "313           0.05969        0.013670             0.008907         0.1833   \n",
       "534           0.09752        0.052630             0.027880         0.1619   \n",
       "319           0.03454        0.013420             0.016990         0.1472   \n",
       "7             0.16450        0.093660             0.059850         0.2196   \n",
       "393           0.20870        0.281000             0.156200         0.2162   \n",
       "141           0.11370        0.094470             0.059430         0.1861   \n",
       "86            0.09947        0.120400             0.049380         0.2075   \n",
       "478           0.08228        0.053080             0.019690         0.1779   \n",
       "503           0.12750        0.167600             0.100300         0.1505   \n",
       "215           0.15170        0.099010             0.056020         0.2106   \n",
       "398           0.04768        0.027120             0.007246         0.1535   \n",
       "490           0.05200        0.017140             0.012610         0.1544   \n",
       "252           0.18490        0.241700             0.097400         0.1733   \n",
       "468           0.20040        0.213600             0.100200         0.1696   \n",
       "357           0.05492        0.015020             0.020880         0.1424   \n",
       "254           0.11880        0.137900             0.085910         0.1776   \n",
       "276           0.03872        0.001487             0.003333         0.1954   \n",
       "178           0.01938        0.001595             0.001852         0.1395   \n",
       "281           0.04340        0.022450             0.027630         0.2101   \n",
       "390           0.07542        0.019230             0.019680         0.1800   \n",
       "508           0.06712        0.055260             0.045630         0.1711   \n",
       "129           0.15890        0.254500             0.114900         0.2202   \n",
       "144           0.05139        0.022510             0.007875         0.1399   \n",
       "72            0.18300        0.169200             0.079440         0.1927   \n",
       "235           0.06945        0.014620             0.018960         0.1517   \n",
       "37            0.03766        0.025620             0.029230         0.1467   \n",
       "\n",
       "     mean fractal dimension           ...             worst radius  \\\n",
       "408                 0.06069           ...                   21.080   \n",
       "4                   0.05883           ...                   22.540   \n",
       "307                 0.06833           ...                    9.699   \n",
       "386                 0.06154           ...                   13.130   \n",
       "404                 0.05708           ...                   13.180   \n",
       "434                 0.05703           ...                   16.310   \n",
       "19                  0.05766           ...                   15.110   \n",
       "517                 0.06188           ...                   23.730   \n",
       "535                 0.06251           ...                   24.300   \n",
       "445                 0.06850           ...                   12.980   \n",
       "554                 0.05708           ...                   13.890   \n",
       "236                 0.06309           ...                   31.010   \n",
       "117                 0.06768           ...                   18.810   \n",
       "157                 0.05268           ...                   18.220   \n",
       "162                 0.06082           ...                   26.730   \n",
       "78                  0.08142           ...                   23.370   \n",
       "409                 0.05597           ...                   14.100   \n",
       "484                 0.06259           ...                   17.010   \n",
       "334                 0.05945           ...                   13.350   \n",
       "42                  0.06343           ...                   24.090   \n",
       "173                 0.06669           ...                   11.350   \n",
       "223                 0.06303           ...                   19.560   \n",
       "201                 0.05491           ...                   20.420   \n",
       "133                 0.05723           ...                   17.500   \n",
       "232                 0.05828           ...                   12.360   \n",
       "413                 0.05913           ...                   16.760   \n",
       "514                 0.05915           ...                   17.580   \n",
       "244                 0.06000           ...                   21.650   \n",
       "415                 0.06290           ...                   13.050   \n",
       "562                 0.07152           ...                   17.520   \n",
       "..                      ...           ...                      ...   \n",
       "264                 0.05580           ...                   21.580   \n",
       "209                 0.05526           ...                   17.380   \n",
       "316                 0.05649           ...                   12.850   \n",
       "513                 0.05640           ...                   16.760   \n",
       "313                 0.06100           ...                   12.340   \n",
       "534                 0.06408           ...                   11.620   \n",
       "319                 0.05561           ...                   12.900   \n",
       "7                   0.07451           ...                   17.060   \n",
       "393                 0.06606           ...                   26.230   \n",
       "141                 0.06248           ...                   19.920   \n",
       "86                  0.05636           ...                   16.210   \n",
       "478                 0.06574           ...                   12.400   \n",
       "503                 0.05484           ...                   30.790   \n",
       "215                 0.06916           ...                   15.750   \n",
       "398                 0.06214           ...                   12.680   \n",
       "490                 0.05976           ...                   14.170   \n",
       "252                 0.06697           ...                   25.280   \n",
       "468                 0.07369           ...                   21.570   \n",
       "357                 0.05883           ...                   15.110   \n",
       "254                 0.05647           ...                   25.700   \n",
       "276                 0.05821           ...                   12.200   \n",
       "178                 0.05234           ...                   14.000   \n",
       "281                 0.06113           ...                   13.310   \n",
       "390                 0.06569           ...                   11.380   \n",
       "508                 0.05657           ...                   17.320   \n",
       "129                 0.06113           ...                   22.630   \n",
       "144                 0.05688           ...                   11.950   \n",
       "72                  0.06487           ...                   23.320   \n",
       "235                 0.05835           ...                   15.330   \n",
       "37                  0.05863           ...                   13.300   \n",
       "\n",
       "     worst texture  worst perimeter  worst area  worst smoothness  \\\n",
       "408          25.41           138.10      1349.0           0.14820   \n",
       "4            16.67           152.20      1575.0           0.13740   \n",
       "307          20.07            60.90       285.5           0.09861   \n",
       "386          19.29            87.65       529.9           0.10260   \n",
       "404          16.85            84.11       533.1           0.10480   \n",
       "434          20.54           102.30       777.5           0.12180   \n",
       "19           19.26            99.70       711.2           0.14400   \n",
       "517          25.23           160.50      1646.0           0.14170   \n",
       "535          25.48           160.20      1809.0           0.12680   \n",
       "445          30.36            84.48       513.9           0.13110   \n",
       "554          35.74            88.84       595.7           0.12270   \n",
       "236          34.51           206.00      2944.0           0.14810   \n",
       "117          27.37           127.10      1095.0           0.18780   \n",
       "157          28.07           120.30      1032.0           0.08774   \n",
       "162          26.39           174.90      2232.0           0.14380   \n",
       "78           31.72           170.30      1623.0           0.16390   \n",
       "409          28.88            89.00       610.2           0.12400   \n",
       "484          14.20           112.50       854.3           0.15410   \n",
       "334          28.46            84.53       544.3           0.12220   \n",
       "42           33.17           177.40      1651.0           0.12470   \n",
       "173          16.82            72.01       396.5           0.12160   \n",
       "223          30.29           125.90      1088.0           0.15520   \n",
       "201          25.84           139.50      1239.0           0.13810   \n",
       "133          19.25           114.30       922.8           0.12230   \n",
       "232          41.78            78.44       470.9           0.09994   \n",
       "413          31.55           110.20       867.1           0.10770   \n",
       "514          28.06           113.80       967.0           0.12460   \n",
       "244          30.53           144.90      1417.0           0.14630   \n",
       "415          27.21            85.09       522.9           0.14260   \n",
       "562          42.79           128.70       915.0           0.14170   \n",
       "..             ...              ...         ...               ...   \n",
       "264          29.33           140.50      1436.0           0.15580   \n",
       "209          15.92           113.70       932.7           0.12220   \n",
       "316          16.47            81.60       513.1           0.10010   \n",
       "513          17.24           108.50       862.0           0.12230   \n",
       "313          12.87            81.23       467.8           0.10920   \n",
       "534          26.51            76.43       407.5           0.14280   \n",
       "319          20.21            81.76       515.9           0.08409   \n",
       "7            28.14           110.60       897.0           0.16540   \n",
       "393          28.74           172.00      2081.0           0.15020   \n",
       "141          25.27           129.00      1233.0           0.13140   \n",
       "86           29.25           108.40       808.9           0.13060   \n",
       "478          21.90            82.04       467.6           0.13520   \n",
       "503          23.87           211.50      2782.0           0.11990   \n",
       "215          26.93           104.40       750.1           0.14600   \n",
       "398          20.35            80.79       496.7           0.11200   \n",
       "490          31.99            92.74       622.9           0.12560   \n",
       "252          25.59           159.80      1933.0           0.17100   \n",
       "468          28.87           143.60      1437.0           0.12070   \n",
       "357          25.58            96.74       694.4           0.11530   \n",
       "254          24.57           163.10      1972.0           0.14970   \n",
       "276          18.99            77.37       458.0           0.12590   \n",
       "178          29.02            88.18       608.8           0.08125   \n",
       "281          18.26            84.70       533.7           0.10360   \n",
       "390          15.65            73.23       394.5           0.13430   \n",
       "508          17.76           109.80       928.2           0.13540   \n",
       "129          33.58           148.70      1589.0           0.12750   \n",
       "144          20.72            77.79       441.2           0.10760   \n",
       "72           33.82           151.60      1681.0           0.15850   \n",
       "235          30.28            98.27       715.5           0.12870   \n",
       "37           22.81            84.46       545.9           0.09701   \n",
       "\n",
       "     worst compactness  worst concavity  worst concave points  worst symmetry  \\\n",
       "408            0.37350         0.330100              0.197400          0.3060   \n",
       "4              0.20500         0.400000              0.162500          0.2364   \n",
       "307            0.05232         0.014720              0.013890          0.2991   \n",
       "386            0.24310         0.307600              0.091400          0.2677   \n",
       "404            0.06744         0.049210              0.047930          0.2298   \n",
       "434            0.15500         0.122000              0.079710          0.2525   \n",
       "19             0.17730         0.239000              0.128800          0.2977   \n",
       "517            0.33090         0.418500              0.161300          0.2549   \n",
       "535            0.31350         0.443300              0.214800          0.3077   \n",
       "445            0.18220         0.160900              0.120200          0.2599   \n",
       "554            0.16200         0.243900              0.064930          0.2372   \n",
       "236            0.41260         0.582000              0.259300          0.3103   \n",
       "117            0.44800         0.470400              0.202700          0.3585   \n",
       "157            0.17100         0.188200              0.084360          0.2527   \n",
       "162            0.38460         0.681000              0.224700          0.3643   \n",
       "78             0.61640         0.768100              0.250800          0.5440   \n",
       "409            0.17950         0.137700              0.095320          0.3455   \n",
       "484            0.29790         0.400400              0.145200          0.2557   \n",
       "334            0.09052         0.036190              0.039830          0.2554   \n",
       "42             0.74440         0.724200              0.249300          0.4670   \n",
       "173            0.08240         0.039380              0.043060          0.1902   \n",
       "223            0.44800         0.397600              0.147900          0.3993   \n",
       "201            0.34200         0.350800              0.193900          0.2928   \n",
       "133            0.19490         0.170900              0.137400          0.2723   \n",
       "232            0.06885         0.023180              0.030020          0.2911   \n",
       "413            0.33450         0.311400              0.130800          0.3163   \n",
       "514            0.21010         0.286600              0.112000          0.2282   \n",
       "244            0.29680         0.345800              0.156400          0.2920   \n",
       "415            0.21870         0.116400              0.082630          0.3075   \n",
       "562            0.79170         1.170000              0.235600          0.4089   \n",
       "..                 ...              ...                   ...             ...   \n",
       "264            0.25670         0.388900              0.198400          0.3216   \n",
       "209            0.21860         0.296200              0.103500          0.2320   \n",
       "316            0.05332         0.041160              0.018520          0.2293   \n",
       "513            0.19280         0.249200              0.091860          0.2626   \n",
       "313            0.16260         0.083240              0.047150          0.3390   \n",
       "534            0.25100         0.212300              0.098610          0.2289   \n",
       "319            0.04712         0.022370              0.028320          0.1901   \n",
       "7              0.36820         0.267800              0.155600          0.3196   \n",
       "393            0.57170         0.705300              0.242200          0.3828   \n",
       "141            0.22360         0.280200              0.121600          0.2792   \n",
       "86             0.19760         0.334900              0.122500          0.3020   \n",
       "478            0.20100         0.259600              0.074310          0.2941   \n",
       "503            0.36250         0.379400              0.226400          0.2908   \n",
       "215            0.43700         0.463600              0.165400          0.3630   \n",
       "398            0.18790         0.207900              0.055560          0.2590   \n",
       "490            0.18040         0.123000              0.063350          0.3100   \n",
       "252            0.59550         0.848900              0.250700          0.2749   \n",
       "468            0.47850         0.516500              0.199600          0.2301   \n",
       "357            0.10080         0.052850              0.055560          0.2362   \n",
       "254            0.31610         0.431700              0.199900          0.3379   \n",
       "276            0.07348         0.004955              0.011110          0.2758   \n",
       "178            0.03432         0.007977              0.009259          0.2295   \n",
       "281            0.08500         0.067350              0.082900          0.3101   \n",
       "390            0.16500         0.086150              0.066960          0.2937   \n",
       "508            0.13610         0.194700              0.135700          0.2300   \n",
       "129            0.38610         0.567300              0.173200          0.3305   \n",
       "144            0.12230         0.097550              0.034130          0.2300   \n",
       "72             0.73940         0.656600              0.189900          0.3313   \n",
       "235            0.15130         0.062310              0.079630          0.2226   \n",
       "37             0.04619         0.048330              0.050130          0.1987   \n",
       "\n",
       "     worst fractal dimension  \n",
       "408                  0.08503  \n",
       "4                    0.07678  \n",
       "307                  0.07804  \n",
       "386                  0.08824  \n",
       "404                  0.05974  \n",
       "434                  0.06827  \n",
       "19                   0.07259  \n",
       "517                  0.09136  \n",
       "535                  0.07569  \n",
       "445                  0.08251  \n",
       "554                  0.07242  \n",
       "236                  0.08677  \n",
       "117                  0.10650  \n",
       "157                  0.05972  \n",
       "162                  0.09223  \n",
       "78                   0.09964  \n",
       "409                  0.06896  \n",
       "484                  0.08181  \n",
       "334                  0.07207  \n",
       "42                   0.10380  \n",
       "173                  0.07313  \n",
       "223                  0.10640  \n",
       "201                  0.07867  \n",
       "133                  0.07071  \n",
       "232                  0.07307  \n",
       "413                  0.09251  \n",
       "514                  0.06954  \n",
       "244                  0.07614  \n",
       "415                  0.07351  \n",
       "562                  0.14090  \n",
       "..                       ...  \n",
       "264                  0.07570  \n",
       "209                  0.07474  \n",
       "316                  0.06037  \n",
       "513                  0.07048  \n",
       "313                  0.07434  \n",
       "534                  0.08278  \n",
       "319                  0.05932  \n",
       "7                    0.11510  \n",
       "393                  0.10070  \n",
       "141                  0.08158  \n",
       "86                   0.06846  \n",
       "478                  0.09180  \n",
       "503                  0.07277  \n",
       "215                  0.10590  \n",
       "398                  0.09158  \n",
       "490                  0.08203  \n",
       "252                  0.12970  \n",
       "468                  0.12240  \n",
       "357                  0.07113  \n",
       "254                  0.08950  \n",
       "276                  0.06386  \n",
       "178                  0.05843  \n",
       "281                  0.06688  \n",
       "390                  0.07722  \n",
       "508                  0.07230  \n",
       "129                  0.08465  \n",
       "144                  0.06769  \n",
       "72                   0.13390  \n",
       "235                  0.07617  \n",
       "37                   0.06169  \n",
       "\n",
       "[455 rows x 30 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "load = load_breast_cancer()\n",
    "cancer = pd.DataFrame(load.data, columns=load.feature_names)\n",
    "target = pd.DataFrame(load.target, columns =['target'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(cancer, target, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_missing = [col for col in cancer.columns if (cancer[col].isnull().any())]\n",
    "cols_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train datasets\n",
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- worst perimeter <= 106.05\n",
      "|   |--- worst concave points <= 0.16\n",
      "|   |   |--- worst concave points <= 0.14\n",
      "|   |   |   |--- area error <= 48.98\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- area error >  48.98\n",
      "|   |   |   |   |--- worst radius <= 13.55\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- worst radius >  13.55\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |--- worst concave points >  0.14\n",
      "|   |   |   |--- worst texture <= 29.45\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- worst texture >  29.45\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |--- worst concave points >  0.16\n",
      "|   |   |--- worst texture <= 24.78\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- worst texture >  24.78\n",
      "|   |   |   |--- class: 0\n",
      "|--- worst perimeter >  106.05\n",
      "|   |--- worst texture <= 20.65\n",
      "|   |   |--- worst perimeter <= 116.80\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- worst perimeter >  116.80\n",
      "|   |   |   |--- mean smoothness <= 0.08\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- mean smoothness >  0.08\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |--- worst texture >  20.65\n",
      "|   |   |--- mean concave points <= 0.05\n",
      "|   |   |   |--- concave points error <= 0.01\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- concave points error >  0.01\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- mean concave points >  0.05\n",
      "|   |   |   |--- mean smoothness <= 0.08\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- mean smoothness >  0.08\n",
      "|   |   |   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "tree = export_text(classifier, feature_names = list(cancer.columns))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give prediction to training data so we can see how good the model is. Compared to true value of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac {Number of Correct predictions}{Total number of predictions made}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train_score = accuracy_score(y_train, predict_train)\n",
    "acc_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 score Metrics. The metrics used based on this confusion matrix. F1 Score is the **_Harmonic Mean_** between precision and recall.\n",
    "<!-- ![Alt Text](image path \"title\") -->\n",
    "![Alt Text](https://miro.medium.com/max/700/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg \"Confusion Matrix\")\n",
    "\n",
    "$$\n",
    "Precision = \\frac {TruePositives}{TruePositives+FalsePositives}\n",
    "$$ <br>\n",
    "$$\n",
    "Recall = \\frac {TruePositives}{TruePositives+FalseNegatives}\n",
    "$$ <br>\n",
    "$$\n",
    "F1 = 2*(\\frac{1}{\\frac {1}{Precision} + \\frac {1}{Recall}}) = 2*(\\frac {TruePositives}{2TruePositives+FalsePositives+FalseNegatives})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train_score = f1_score(y_train, predict_train)\n",
    "f1_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_valid = classifier.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid_score = accuracy_score(y_valid, predict_valid)\n",
    "acc_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594594594594595"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid_score = f1_score(y_valid, predict_valid)\n",
    "f1_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id3Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "worst perimeter <=105.15\n",
      "|   worst concave points <=0.14\n",
      "|   |   radius error <=0.64: 1 (249) \n",
      "|   |   radius error >0.64\n",
      "|   |   |   mean radius <=12.27: 0 (1) \n",
      "|   |   |   mean radius >12.27: 1 (2) \n",
      "|   worst concave points >0.14\n",
      "|   |   worst texture <=25.94: 1 (6) \n",
      "|   |   worst texture >25.94\n",
      "|   |   |   mean compactness <=0.12\n",
      "|   |   |   |   mean radius <=14.06: 1 (2) \n",
      "|   |   |   |   mean radius >14.06: 0 (1) \n",
      "|   |   |   mean compactness >0.12: 0 (5) \n",
      "worst perimeter >105.15\n",
      "|   worst concave points <=0.15\n",
      "|   |   worst texture <=19.91: 1 (13) \n",
      "|   |   worst texture >19.91\n",
      "|   |   |   worst radius <=16.80\n",
      "|   |   |   |   mean smoothness <=0.09: 1 (8) \n",
      "|   |   |   |   mean smoothness >0.09\n",
      "|   |   |   |   |   smoothness error <=0.00: 1 (2) \n",
      "|   |   |   |   |   smoothness error >0.00: 0 (5) \n",
      "|   |   |   worst radius >16.80\n",
      "|   |   |   |   worst concavity <=0.21\n",
      "|   |   |   |   |   mean texture <=21.26: 1 (2) \n",
      "|   |   |   |   |   mean texture >21.26: 0 (2) \n",
      "|   |   |   |   worst concavity >0.21: 0 (21) \n",
      "|   worst concave points >0.15\n",
      "|   |   mean texture <=15.35\n",
      "|   |   |   mean radius <=14.89: 1 (1) \n",
      "|   |   |   mean radius >14.89: 0 (3) \n",
      "|   |   mean texture >15.35: 0 (132) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import six\n",
    "import sys\n",
    "sys.modules['sklearn.externals.six'] = six\n",
    "\n",
    "from id3 import Id3Estimator, export\n",
    "\n",
    "id3_model1 = Id3Estimator()\n",
    "\n",
    "id3_model1.fit(X_train, y_train.values.ravel()) # make it numpy 1D array, not a df\n",
    "\n",
    "# export text\n",
    "tree_text1 = export.export_text(id3_model1.tree_, feature_names=cancer.columns)\n",
    "\n",
    "print(tree_text1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "predict_train1 = id3_model1.predict(X_train)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_train1, y_train))\n",
    "print(\"F1 score: \", f1_score(predict_train1, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to test data with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9385964912280702\n",
      "F1 score:  0.953020134228188\n"
     ]
    }
   ],
   "source": [
    "# Model Prediction\n",
    "predict_test1 = id3_model1.predict(X_valid)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_test1,y_valid))\n",
    "print(\"F1 score: \", f1_score(predict_test1,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine how many clusters\n",
    "n_clust = y_train['target'].nunique()\n",
    "\n",
    "kmeans_model_1 = KMeans(n_clusters=n_clust,random_state=1)\n",
    "kmeans_model_1.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the data train and data test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1\n",
      " 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
      " 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 1 0 1 0 0]\n",
      "     target\n",
      "408       0\n",
      "4         0\n",
      "307       1\n",
      "386       1\n",
      "404       1\n",
      "434       1\n",
      "19        1\n",
      "517       0\n",
      "535       0\n",
      "445       1\n",
      "554       1\n",
      "236       0\n",
      "117       0\n",
      "157       1\n",
      "162       0\n",
      "78        0\n",
      "409       1\n",
      "484       1\n",
      "334       1\n",
      "42        0\n",
      "173       1\n",
      "223       0\n",
      "201       0\n",
      "133       1\n",
      "232       1\n",
      "413       1\n",
      "514       0\n",
      "244       0\n",
      "415       1\n",
      "562       0\n",
      "..      ...\n",
      "264       0\n",
      "209       1\n",
      "316       1\n",
      "513       1\n",
      "313       1\n",
      "534       1\n",
      "319       1\n",
      "7         0\n",
      "393       0\n",
      "141       0\n",
      "86        0\n",
      "478       1\n",
      "503       0\n",
      "215       0\n",
      "398       1\n",
      "490       1\n",
      "252       0\n",
      "468       0\n",
      "357       1\n",
      "254       0\n",
      "276       1\n",
      "178       1\n",
      "281       1\n",
      "390       1\n",
      "508       1\n",
      "129       0\n",
      "144       1\n",
      "72        0\n",
      "235       1\n",
      "37        1\n",
      "\n",
      "[455 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "kmeans_predict_train = kmeans_model_1.predict(X_train)\n",
    "kmeans_predict_test = kmeans_model_1.predict(X_valid)\n",
    "print(kmeans_predict_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to switch the value from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def switch(x: int) -> int:\n",
    "    if (x==0):\n",
    "        return x+1\n",
    "    elif (x==1):\n",
    "        return x-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation with training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.15164835164835164\n",
      "First clusterization F1: 0.005154639175257732\n",
      "Second clusterization accuracy: 0.8483516483516483\n",
      "Second clusterization F1: 0.891679748822606\n"
     ]
    }
   ],
   "source": [
    "# Find out each cluster belong with which class\n",
    "\n",
    "# First clusterization using first prediction (k_means_train)\n",
    "# Calculate the accuracy and F1\n",
    "acc_train = accuracy_score(y_train, kmeans_predict_train)\n",
    "f1_train = f1_score(y_train, kmeans_predict_train)\n",
    "print(f\"First clusterization accuracy: {acc_train}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_train = pd.Series(kmeans_predict_train)\n",
    "fix = switched_train.apply(switch)\n",
    "acc_train1= accuracy_score(y_train, fix)\n",
    "f1_train2= f1_score(y_train, fix)\n",
    "print(f\"Second clusterization accuracy: {acc_train1}\")\n",
    "print(f\"Second clusterization F1: {f1_train2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance can be seen by the greatest metrics score. So the cluster 0 belongs to class of 1, cluster 1 belongs to class of 0. Now, for evaluation with validation data (datatest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.19298245614035087\n",
      "First clusterization F1: 0.005154639175257732\n",
      "Second clusterization accuracy: 0.8070175438596491\n",
      "Second clusterization F1: 0.8674698795180723\n"
     ]
    }
   ],
   "source": [
    "# First clusterization\n",
    "acc_valid = accuracy_score(y_valid, kmeans_predict_test)\n",
    "f1_valid = f1_score(y_valid, kmeans_predict_test)\n",
    "print(f\"First clusterization accuracy: {acc_valid}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_valid = pd.Series(kmeans_predict_test).apply(switch)\n",
    "acc_valid_switch = accuracy_score(y_valid, switched_valid)\n",
    "f1_valid_switch = f1_score(y_valid, switched_valid)\n",
    "print(f\"Second clusterization accuracy: {acc_valid_switch}\")\n",
    "print(f\"Second clusterization F1: {f1_valid_switch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as when we evaluate with training data, the second clusterization is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\annisa rahim\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Data\n",
      "Accuracy:  0.945054945054945\n",
      "F1 score:  0.9566724436741767\n"
     ]
    }
   ],
   "source": [
    "predict_train = model.predict(X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Local Data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data\n",
      "Accuracy:  0.9473684210526315\n",
      "F1 score:  0.9589041095890412\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the dataset (using Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling untuk mencapai konvergensi dari data (menghindari ConvergenceWarning)\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling the scaled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to train data with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Data\n",
      "Accuracy:  0.9912087912087912\n",
      "F1 score:  0.993006993006993\n"
     ]
    }
   ],
   "source": [
    "predict_train = model.predict(X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Local Data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to test data with Accuracy and F1 score metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data\n",
      "Accuracy:  0.9736842105263158\n",
      "F1 score:  0.9793103448275863\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data (Using MinMax Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "load = load_breast_cancer()\n",
    "cancer = pd.DataFrame(load.data, columns=load.feature_names)\n",
    "target = pd.DataFrame(load.target, columns =['target'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(cancer, target, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Data\n",
      "Accuracy:  0.9626373626373627\n",
      "F1 score:  0.9709401709401708\n"
     ]
    }
   ],
   "source": [
    "predict_train = model.predict(X_train)\n",
    "\n",
    "acc = accuracy_score(y_train, predict_train)\n",
    "f1 = f1_score(y_train, predict_train)\n",
    "\n",
    "print(\"Local Data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data\n",
      "Accuracy:  0.9736842105263158\n",
      "F1 score:  0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(X_valid)\n",
    "\n",
    "acc = accuracy_score(y_valid, predict_test)\n",
    "f1 = f1_score(y_valid, predict_test)\n",
    "\n",
    "print(\"Test Data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kesimpulan\n",
    "\n",
    "no scaler: (convergence warning)\n",
    "- Accuracy:  0.9473684210526315\n",
    "- F1 score:  0.9589041095890412\n",
    "\n",
    "with scaling \n",
    "- standard\n",
    "    - Accuracy:  0.9736842105263158\n",
    "    - F1 score:  0.9793103448275863\n",
    "- minmax\n",
    "    - Accuracy:  0.9736842105263158\n",
    "    - F1 score:  0.9795918367346939\n",
    "    \n",
    "This data is better with scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neutral_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the SVM (Support Vector Machines) constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the traning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, y_train.values.ravel())\n",
    "# .values will give the values in an array (shape: (n,1)\n",
    "# .ravel will convert that array shape to (n, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_svm_breast = svm.predict(X_train)\n",
    "pred_val_svm_breast = svm.predict(X_valid)\n",
    "pred_val_svm_breast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate data train prediction with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9824175824175824\n",
      "F1 :  0.986013986013986\n"
     ]
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_train, pred_train_svm_breast)\n",
    "f1 = f1_score(y_train, pred_train_svm_breast)\n",
    "print(\"Accuracy : \", acc_score)\n",
    "print(\"F1 : \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate data test prediction with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9736842105263158\n",
      "F1 :  0.9793103448275863\n"
     ]
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_valid, pred_val_svm_breast)\n",
    "f1 = f1_score(y_valid, pred_val_svm_breast)\n",
    "print(\"Accuracy : \", acc_score)\n",
    "print(\"F1 : \",f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play-Tennis Dataset\n",
    "\n",
    "Load the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  play\n",
       "3  yes\n",
       "7   no\n",
       "6  yes"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis = pd.read_csv('datasets/PlayTennis.csv')\n",
    "X_tennis = tennis.drop(['play'],axis=1)\n",
    "y_tennis = pd.DataFrame(tennis['play'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train_tennis, X_valid_tennis, y_train_tennis, y_valid_tennis = train_test_split(X_tennis, y_tennis, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "y_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_missing_tennis = [col for col in tennis.columns if tennis[col].isnull().any()]\n",
    "cols_with_missing_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train datasets\n",
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the categorical variable to numeric variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outlook</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outlook  temp  humidity  windy\n",
       "2         0     1         0  False\n",
       "10        2     2         1   True\n",
       "4         1     0         1  False\n",
       "1         2     1         0   True\n",
       "12        0     1         1  False\n",
       "0         2     1         0  False\n",
       "13        1     2         0   True\n",
       "9         1     2         1  False\n",
       "8         2     0         1  False\n",
       "11        0     2         0   True\n",
       "5         1     0         1   True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cond = (X_train_tennis.dtypes == 'object')\n",
    "object_cols = list(cond[cond].index)\n",
    "\n",
    "label_X_train = X_train_tennis.copy()\n",
    "label_X_valid = X_valid_tennis.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train_tennis[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid_tennis[col])\n",
    "\n",
    "# In case needed\n",
    "label_y_train = label_encoder.fit_transform(y_train_tennis.values.ravel())\n",
    "label_y_valid = label_encoder.transform(y_valid_tennis.values.ravel())\n",
    "\n",
    "label_X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis_classifier = DecisionTreeClassifier(random_state=1)\n",
    "tennis_classifier.fit(label_X_train, y_train_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- outlook <= 0.50\n",
      "|   |--- class: yes\n",
      "|--- outlook >  0.50\n",
      "|   |--- humidity <= 0.50\n",
      "|   |   |--- class: no\n",
      "|   |--- humidity >  0.50\n",
      "|   |   |--- windy <= 0.50\n",
      "|   |   |   |--- class: yes\n",
      "|   |   |--- windy >  0.50\n",
      "|   |   |   |--- outlook <= 1.50\n",
      "|   |   |   |   |--- class: no\n",
      "|   |   |   |--- outlook >  1.50\n",
      "|   |   |   |   |--- class: yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree_tennis = export_text(tennis_classifier, feature_names = list(X_tennis.columns))\n",
    "print(tree_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict training data with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes',\n",
       "       'no'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_train_tennis = tennis_classifier.predict(label_X_train)\n",
    "predict_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to training data with Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_tennis = accuracy_score(y_train_tennis,predict_train_tennis)\n",
    "acc_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to training data with F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_train_tennis = f1_score(y_train_tennis,predict_train_tennis,pos_label='yes')\n",
    "f1_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the test data using model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no', 'no', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_valid_tennis = tennis_classifier.predict(label_X_valid)\n",
    "predict_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with datatest using Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid_tennis = accuracy_score(y_valid_tennis,predict_valid_tennis)\n",
    "acc_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with datatest using F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid_tennis = f1_score(y_valid_tennis,predict_valid_tennis,pos_label='yes')\n",
    "f1_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Id3Estimator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Train the model and print the tree produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outlook <=0.50: 1 (3) \n",
      "outlook >0.50\n",
      "|   humidity <=0.50: 0 (3) \n",
      "|   humidity >0.50\n",
      "|   |   windy <=0.50: 1 (3) \n",
      "|   |   windy >0.50\n",
      "|   |   |   temp <=1.00: 0 (1) \n",
      "|   |   |   temp >1.00: 1 (1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "id3_model2 = Id3Estimator()\n",
    "id3_model2.fit(label_X_train, label_y_train)\n",
    "\n",
    "tree_text2 = export.export_text(id3_model2.tree_, feature_names=list(tennis.columns))\n",
    "print(tree_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to training data with Accuracy and F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0\n",
      "F1 score:  1.0\n"
     ]
    }
   ],
   "source": [
    "predict_train2 = id3_model2.predict(label_X_train)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_train2,label_y_train))\n",
    "print(\"F1 score: \", f1_score(predict_train2,label_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to test data with using Accuracy and F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "predict_test2 = id3_model2.predict(label_X_valid)\n",
    "\n",
    "print(\"Accuracy: \", accuracy_score(predict_test2,label_y_valid))\n",
    "print(\"F1 score: \", f1_score(predict_test2,label_y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define KMeans and then train the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2, random_state=1)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Determine how many clusters\n",
    "n_clst = 2\n",
    "# Define the KMeans\n",
    "kmeans_model_2 = KMeans(n_clusters = n_clst, random_state=1)\n",
    "kmeans_model_2.fit(label_X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and produce clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = kmeans_model_2.predict(label_X_train)\n",
    "predict_test = kmeans_model_2.predict(label_X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the training data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.36363636363636365\n",
      "First clusterization F1: 0.4615384615384615\n",
      "Second clusterization accuracy: 0.6363636363636364\n",
      "Second clusterization F1: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Find out each cluster belong with which class\n",
    "\n",
    "# First clusterization using first prediction (k_means_train)\n",
    "# Calculate the accuracy and F1\n",
    "acc_train = accuracy_score(label_y_train, predict_train)\n",
    "f1_train = f1_score(label_y_train, predict_train)\n",
    "print(f\"First clusterization accuracy: {acc_train}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_train = pd.Series(predict_train)\n",
    "fix = switched_train.apply(switch)\n",
    "acc_train1= accuracy_score(label_y_train, fix)\n",
    "f1_train2= f1_score(label_y_train, fix)\n",
    "print(f\"Second clusterization accuracy: {acc_train1}\")\n",
    "print(f\"Second clusterization F1: {f1_train2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First clusterization is better, so we take the metrics as the final metrics. Evaluate the validation data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First clusterization accuracy: 0.0\n",
      "First clusterization F1: 0.4615384615384615\n",
      "Second clusterization accuracy: 1.0\n",
      "Second clusterization F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# First clusterization\n",
    "acc_valid = accuracy_score(label_y_valid, predict_test)\n",
    "f1_valid = f1_score(label_y_valid, predict_test)\n",
    "print(f\"First clusterization accuracy: {acc_valid}\")\n",
    "print(f\"First clusterization F1: {f1_train}\")\n",
    "\n",
    "# Second clusterization: switch 0 to 1\n",
    "switched_valid = pd.Series(predict_test).apply(switch)\n",
    "acc_valid_switch = accuracy_score(label_y_valid, switched_valid)\n",
    "f1_valid_switch = f1_score(label_y_valid, switched_valid)\n",
    "print(f\"Second clusterization accuracy: {acc_valid_switch}\")\n",
    "print(f\"Second clusterization F1: {f1_valid_switch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as when we evaluate with training data, the first clusterization is better and then will be our final metrics score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the train data and evaluate the prediction with Accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local data\n",
      "Accuracy:  0.8181818181818182\n",
      "F1 score:  0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "predict_train = model.predict(label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"local data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"test data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling : Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaling if needed\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "label_X_train = scaler.fit_transform(label_X_train)\n",
    "label_X_valid = scaler.fit_transform(label_X_valid)\n",
    "\n",
    "model.fit(label_X_train,label_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local data\n",
      "Accuracy:  0.9090909090909091\n",
      "F1 score:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "predict_train = model.predict(label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"local data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"test data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling : MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tennis = pd.read_csv('datasets/PlayTennis.csv')\n",
    "X_tennis = tennis.drop(['play'],axis=1)\n",
    "y_tennis = pd.DataFrame(tennis['play'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train_tennis, X_valid_tennis, y_train_tennis, y_valid_tennis = train_test_split(X_tennis, y_tennis, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "y_valid_tennis\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cond = (X_train_tennis.dtypes == 'object')\n",
    "object_cols = list(cond[cond].index)\n",
    "\n",
    "label_X_train = X_train_tennis.copy()\n",
    "label_X_valid = X_valid_tennis.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train_tennis[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid_tennis[col])\n",
    "\n",
    "# In case needed\n",
    "label_y_train = label_encoder.fit_transform(y_train_tennis.values.ravel())\n",
    "label_y_valid = label_encoder.transform(y_valid_tennis.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local data\n",
      "Accuracy:  0.8181818181818182\n",
      "F1 score:  0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "predict_train = model.predict(label_X_train)\n",
    "\n",
    "acc = accuracy_score(label_y_train, predict_train)\n",
    "f1 = f1_score(label_y_train, predict_train)\n",
    "\n",
    "print(\"local data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data\n",
      "Accuracy:  0.6666666666666666\n",
      "F1 score:  0.8\n"
     ]
    }
   ],
   "source": [
    "predict_test = model.predict(label_X_valid)\n",
    "\n",
    "acc = accuracy_score(label_y_valid, predict_test)\n",
    "f1 = f1_score(label_y_valid, predict_test)\n",
    "\n",
    "print(\"test data\")\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCORE LOCAL DATA\n",
    "\n",
    "tanpa scaling:\n",
    "- Accuracy:  0.8181818181818182\n",
    "- F1 score:  0.8750000000000001\n",
    "\n",
    "StandardScaler:\n",
    "- Accuracy:  0.9090909090909091\n",
    "- F1 score:  0.9333333333333333\n",
    "\n",
    "MinMaxScaler:\n",
    "- Accuracy:  0.8181818181818182\n",
    "- F1 score:  0.8750000000000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCORE TEST DATA\n",
    "- Accuracy:  0.6666666666666666\n",
    "- F1 score:  0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kesimpulan\n",
    "\n",
    "Local data better with scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the SVM (Support Vector Machines) constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(label_X_train, y_train_tennis.values.ravel())\n",
    "# .values will give the values in an array (shape: (n,1)\n",
    "# .ravel will convert that array shape to (n, )\n",
    "# y_train_tennis.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the data train and the data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'yes', 'yes', 'no', 'yes', 'no', 'yes', 'yes', 'yes', 'yes',\n",
       "       'yes'], dtype=object)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_svm_train_tennis = svm.predict(label_X_train)\n",
    "pred_svm_val_tennis = svm.predict(label_X_valid)\n",
    "pred_svm_train_tennis\n",
    "# pred_svm_val_tennis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the prediction of data train with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8181818181818182\n",
      "F1 :  0.8750000000000001\n"
     ]
    }
   ],
   "source": [
    "acc_score_svm = accuracy_score(y_train_tennis, pred_svm_train_tennis)\n",
    "# must define the positive label = yes\n",
    "f1_score_svm = f1_score(y_train_tennis, pred_svm_train_tennis, pos_label=\"yes\")\n",
    "print(\"Accuracy : \",acc_score_svm)\n",
    "print(\"F1 : \",f1_score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the prediction of data validation with using Accuracy and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  1.0\n",
      "F1 :  1.0\n"
     ]
    }
   ],
   "source": [
    "acc_score_svm = accuracy_score(y_valid_tennis, pred_svm_val_tennis)\n",
    "# must define the positive label = yes\n",
    "f1_score_svm = f1_score(y_valid_tennis, pred_svm_val_tennis, pos_label=\"yes\")\n",
    "print(\"Accuracy : \",acc_score_svm)\n",
    "print(\"F1 : \",f1_score_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis accuracy and F1 score for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
