{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Made by:\n",
    "- Annisa Rahim / 13518089\n",
    "- Stefanus Gusega Gunawan / 13518149"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast-Cancer Dataset\n",
    "\n",
    "Load the datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "load = load_breast_cancer()\n",
    "cancer = pd.DataFrame(load.data, columns=load.feature_names)\n",
    "target = pd.DataFrame(load.target, columns =['target'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(cancer, target, train_size = 0.8, test_size = 0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_missing = [col for col in cancer.columns if (cancer[col].isnull().any())]\n",
    "cols_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train datasets\n",
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(random_state=1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- worst perimeter <= 106.05\n",
      "|   |--- worst concave points <= 0.16\n",
      "|   |   |--- worst concave points <= 0.14\n",
      "|   |   |   |--- area error <= 48.98\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- area error >  48.98\n",
      "|   |   |   |   |--- worst radius <= 13.55\n",
      "|   |   |   |   |   |--- class: 0\n",
      "|   |   |   |   |--- worst radius >  13.55\n",
      "|   |   |   |   |   |--- class: 1\n",
      "|   |   |--- worst concave points >  0.14\n",
      "|   |   |   |--- worst texture <= 29.45\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- worst texture >  29.45\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |--- worst concave points >  0.16\n",
      "|   |   |--- worst texture <= 24.78\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- worst texture >  24.78\n",
      "|   |   |   |--- class: 0\n",
      "|--- worst perimeter >  106.05\n",
      "|   |--- worst texture <= 20.65\n",
      "|   |   |--- worst perimeter <= 116.80\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- worst perimeter >  116.80\n",
      "|   |   |   |--- mean smoothness <= 0.08\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- mean smoothness >  0.08\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |--- worst texture >  20.65\n",
      "|   |   |--- mean concave points <= 0.05\n",
      "|   |   |   |--- concave points error <= 0.01\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- concave points error >  0.01\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |--- mean concave points >  0.05\n",
      "|   |   |   |--- mean smoothness <= 0.08\n",
      "|   |   |   |   |--- class: 1\n",
      "|   |   |   |--- mean smoothness >  0.08\n",
      "|   |   |   |   |--- class: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "\n",
    "tree = export_text(classifier, feature_names = list(cancer.columns))\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give prediction to training data so we can see how good the model is. Compared to true value of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac {Number of Correct predictions}{Total number of predictions made}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# INI AKU GATAU MAKSUD SOALNYA, AKU ASUMSIIN GINI YA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train_score = accuracy_score(y_train, predict_train)\n",
    "acc_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 score Metrics. The metrics used based on this confusion matrix. F1 Score is the **_Harmonic Mean_** between precision and recall.\n",
    "<!-- ![Alt Text](image path \"title\") -->\n",
    "![Alt Text](https://miro.medium.com/max/700/1*OhEnS-T54Cz0YSTl_c3Dwg.jpeg \"Confusion Matrix\")\n",
    "\n",
    "$$\n",
    "Precision = \\frac {TruePositives}{TruePositives+FalsePositives}\n",
    "$$ <br>\n",
    "$$\n",
    "Recall = \\frac {TruePositives}{TruePositives+FalseNegatives}\n",
    "$$ <br>\n",
    "$$\n",
    "F1 = 2*(\\frac{1}{\\frac {1}{Precision} + \\frac {1}{Recall}}) = 2*(\\frac {TruePositives}{2TruePositives+FalsePositives+FalseNegatives})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_train_score = f1_score(y_train, predict_train)\n",
    "f1_train_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the test data (y_valid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_valid = classifier.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9473684210526315"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid_score = accuracy_score(y_valid, predict_valid)\n",
    "acc_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9594594594594595"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid_score = f1_score(y_valid, predict_valid)\n",
    "f1_valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play-Tennis Dataset\n",
    "\n",
    "Load the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  play\n",
       "3  yes\n",
       "7   no\n",
       "6  yes"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis = pd.read_csv('datasets/PlayTennis.csv')\n",
    "X_tennis = tennis.drop(['play'],axis=1)\n",
    "y_tennis = pd.DataFrame(tennis['play'])\n",
    "\n",
    "# Split 80 : 20\n",
    "X_train_tennis, X_valid_tennis, y_train_tennis, y_valid_tennis = train_test_split(X_tennis, y_tennis, train_size = 0.8, test_size = 0.2, random_state=1)\n",
    "y_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether there are missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_with_missing_tennis = [col for col in tennis.columns if tennis[col].isnull().any()]\n",
    "cols_with_missing_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train datasets\n",
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the categorical variable to numeric variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cond = (X_train_tennis.dtypes == 'object')\n",
    "object_cols = list(cond[cond].index)\n",
    "\n",
    "label_X_train = X_train_tennis.copy()\n",
    "label_X_valid = X_valid_tennis.copy()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train_tennis[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid_tennis[col])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tennis_classifier = DecisionTreeClassifier(random_state=1)\n",
    "tennis_classifier.fit(label_X_train, y_train_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- outlook <= 0.50\n",
      "|   |--- class: yes\n",
      "|--- outlook >  0.50\n",
      "|   |--- humidity <= 0.50\n",
      "|   |   |--- class: no\n",
      "|   |--- humidity >  0.50\n",
      "|   |   |--- windy <= 0.50\n",
      "|   |   |   |--- class: yes\n",
      "|   |   |--- windy >  0.50\n",
      "|   |   |   |--- outlook <= 1.50\n",
      "|   |   |   |   |--- class: no\n",
      "|   |   |   |--- outlook >  1.50\n",
      "|   |   |   |   |--- class: yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ini aneh wkwk\n",
    "\n",
    "tree_tennis = export_text(tennis_classifier, feature_names = list(X_tennis.columns))\n",
    "print(tree_tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with training data, so we know whether the model is good enough or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'yes', 'yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'yes',\n",
       "       'no'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_train_tennis = tennis_classifier.predict(label_X_train)\n",
    "predict_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_train_tennis = accuracy_score(y_train_tennis,predict_train_tennis)\n",
    "acc_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the dataset using model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['no', 'no', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_valid_tennis = tennis_classifier.predict(label_X_valid)\n",
    "predict_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_train_tennis = f1_score(y_train_tennis,predict_train_tennis,pos_label='yes')\n",
    "f1_train_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with datatest using Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_valid_tennis = accuracy_score(y_valid_tennis,predict_valid_tennis)\n",
    "acc_valid_tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with datatest using F1 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_valid_tennis = f1_score(y_valid_tennis,predict_valid_tennis,pos_label='yes')\n",
    "f1_valid_tennis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
